{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5d3475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.signal import spectrogram, stft, istft, check_NOLA\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import neptune\n",
    "from neptune.utils import stringify_unsupported\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdef38d8",
   "metadata": {},
   "source": [
    "# PARAMETERS - GENERAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f63db3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "arraySavePath = '/blue/gkalamangalam/jmark.ettinger/predictScalp/trainTestRTheta.npz'\n",
    "modelPath = '/blue/gkalamangalam/jmark.ettinger/predictScalp/pytorchModels/model1.pth'\n",
    "\n",
    "neptuneProject = 'jettinger35/predictScalp'\n",
    "api_token = os.environ.get('NEPTUNE_API_TOKEN')\n",
    "\n",
    "modelLoadFlag = True\n",
    "\n",
    "subsampleFreq = 64   # FINAL FREQUENCY IN HERTZ AFTER SUBSAMPLING\n",
    "secondsInWindow = 1.\n",
    "nperseg = subsampleFreq * secondsInWindow\n",
    "noverlap = nperseg - 1\n",
    "window = ('tukey', .25)\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f7fce9",
   "metadata": {},
   "source": [
    "# PARAMETERS - TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "224077ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5000\n",
    "batch_size = 64\n",
    "learningRate = 1e-5\n",
    "loss_fn = nn.MSELoss()\n",
    "optChoice = 'adam'\n",
    "\n",
    "patience = 50\n",
    "min_delta = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6db8c0",
   "metadata": {},
   "source": [
    "# UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe7e2826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT STFT FROM R,THETA TO COMPLEX\n",
    "# dim(z) = (# timesteps, # freq bins x 2 (2 reals = 1 complex))\n",
    "\n",
    "def rThetaToComplex(z):\n",
    "    rows, cols = z.shape\n",
    "    shortTermFourier = np.zeros((rows, cols // 2), dtype=np.csingle)\n",
    "    for i in range(rows):\n",
    "        for k in range(cols // 2):\n",
    "            r = z[i,k]\n",
    "            theta = z[i, (k + cols // 2)]\n",
    "            shortTermFourier[i,k] =  r * np.exp(complex(0, theta))\n",
    "    return shortTermFourier.transpose() # dim = (# freq bins, # timepoints)\n",
    "\n",
    "# CONVERT REAL STFT TO COMPLEX STFT, INVERT TO GET THE ISTFT (I.E. TIME SERIES), THEN PLOT\n",
    "\n",
    "def realSTFTtoTimeSeries(realSTFT):\n",
    "    shortTermFourierComplex = rThetaToComplex(realSTFT)\n",
    "    times, inverseShortFourier = istft(shortTermFourierComplex, \n",
    "                                       fs=subsampleFreq, \n",
    "                                       window=window, \n",
    "                                       nperseg=nperseg, \n",
    "                                       noverlap=noverlap)\n",
    "    return times, inverseShortFourier\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        return loss\n",
    "            \n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss\n",
    "\n",
    "class EarlyStopper:\n",
    "    def __init__(self, patience, min_delta):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.min_validation_loss = np.inf\n",
    "\n",
    "    def early_stop(self, validation_loss):\n",
    "        if validation_loss < self.min_validation_loss:\n",
    "            self.min_validation_loss = validation_loss\n",
    "            self.counter = 0\n",
    "        elif validation_loss > (self.min_validation_loss + self.min_delta):\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                return True\n",
    "        return False\n",
    "    \n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, layerOrderedDict):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(layerOrderedDict)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbbd749",
   "metadata": {},
   "source": [
    "# LOAD NUMPY DATA ARRAYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cedd53c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: \n",
      "Shape of X [N, C, H, W]: torch.Size([64, 5742])\n",
      "Shape of y: torch.Size([64, 66]) torch.float32\n",
      "\n",
      "test: \n",
      "Shape of X [N, C, H, W]: torch.Size([64, 5742])\n",
      "Shape of y: torch.Size([64, 66]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "npzfile = np.load(arraySavePath)\n",
    "x_trainRTheta = npzfile['x_trainRTheta']\n",
    "x_validRTheta = npzfile['x_validRTheta'] \n",
    "y_trainRTheta = npzfile['y_trainRTheta'] \n",
    "y_validRTheta = npzfile['y_validRTheta']\n",
    "\n",
    "_,nY = y_validRTheta.shape\n",
    "\n",
    "trainXTensor = torch.Tensor(x_trainRTheta)\n",
    "trainYTensor = torch.Tensor(y_trainRTheta)\n",
    "\n",
    "trainDataset = TensorDataset(trainXTensor,trainYTensor)\n",
    "trainDataLoader = DataLoader(trainDataset,batch_size=batch_size, shuffle=True)\n",
    "\n",
    "validXTensor = torch.Tensor(x_validRTheta)\n",
    "validYTensor = torch.Tensor(y_validRTheta)\n",
    "\n",
    "validDataset = TensorDataset(validXTensor,validYTensor)\n",
    "validDataLoader = DataLoader(validDataset,batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "print(\"train: \")\n",
    "for X, y in trainDataLoader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "    \n",
    "print(\"\\ntest: \")\n",
    "for X, y in validDataLoader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5243c05c",
   "metadata": {},
   "source": [
    "# DEFINE OR LOAD THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eeaad9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 4.633556 \n",
      "\n",
      "NeuralNetwork(\n",
      "  (model): Sequential(\n",
      "    (l1): Linear(in_features=5742, out_features=512, bias=True)\n",
      "    (rl1): ReLU()\n",
      "    (l2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (lr2): ReLU()\n",
      "    (l3): Linear(in_features=512, out_features=66, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Define model\n",
    "\n",
    "if modelLoadFlag == True:\n",
    "    model = torch.load(modelPath)\n",
    "    bestTestLoss = test(validDataLoader, model, loss_fn)\n",
    "else:\n",
    "    layerOrderedDict = OrderedDict([('l1', nn.Linear(5742, 512)),\n",
    "                                ('rl1', nn.ReLU()),\n",
    "                                ('l2', nn.Linear(512, 512)),\n",
    "                                ('lr2', nn.ReLU()),\n",
    "                                ('l3', nn.Linear(512, 66))])\n",
    "    \n",
    "    model = NeuralNetwork(layerOrderedDict)\n",
    "    bestTestLoss = float('inf')\n",
    "    \n",
    "    \n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df68e5ed",
   "metadata": {},
   "source": [
    "# TRAIN (LOG DATA TO NEPTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "64c6012d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://new-ui.neptune.ai/jettinger35/predictScalp/e/PRED-12\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 3.695208  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.984244 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 3.687844  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.984945 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 3.512255  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.983126 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.847361  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.981781 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.390279  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.982100 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 4.180506  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.982082 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 3.695006  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.983579 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 3.497055  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.981409 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 3.821937  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.984537 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 3.663380  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.982142 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 3.867507  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.984848 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 4.114589  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.981173 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 3.315607  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.983976 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 3.803423  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.986963 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 3.114330  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.982333 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 3.197932  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.981784 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 3.290063  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.979946 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 3.632184  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.981189 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 3.679275  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.981765 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 3.424538  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.976311 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 3.797554  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.975332 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 3.562507  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.973897 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 3.681291  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.973226 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 3.235701  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.972630 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 3.710685  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.971770 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 3.828701  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.971952 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 3.470690  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.970780 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 3.429116  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.973113 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 3.451781  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.970706 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 3.480213  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.968822 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 3.435004  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.971508 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 3.664413  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.967977 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 3.595080  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.967064 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 3.793232  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.964901 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 4.610123  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.966671 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 3.540358  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.964489 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 3.857689  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.961025 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 3.452029  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.960370 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 3.856046  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.960314 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 4.070487  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.960119 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 3.875466  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.956755 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 4.117852  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.954758 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 4.010211  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.955408 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 3.450473  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.953528 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 3.547712  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.951623 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 3.468576  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.953392 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 3.967669  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.951613 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 3.978535  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.949093 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 3.269776  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.952580 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 2.980016  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.949292 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 4.252286  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.948139 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 4.279271  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.946285 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 3.654075  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.946100 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 3.572026  [   64/89440]\n",
      "File /blue/gkalamangalam/jmark.ettinger/predictScalp/pytorchModels/model1.pth changed during upload, restarting upload.\n",
      "Test Error: \n",
      " Avg loss: 3.945114 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 3.476117  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.946118 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 3.779011  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.947600 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 3.798510  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.949042 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 3.623999  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.943458 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 4.341259  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.942663 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 4.024244  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.943002 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 3.551240  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.942558 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 4.051557  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.941394 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 3.199972  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.942110 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 3.418008  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.940518 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 4.017788  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.941014 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 3.300059  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.940178 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 4.089463  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.939677 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 4.144495  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.940560 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 3.558723  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.938844 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 3.743752  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.938823 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 3.389461  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.941687 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 3.573178  [   64/89440]\n",
      "File /blue/gkalamangalam/jmark.ettinger/predictScalp/pytorchModels/model1.pth changed during upload, restarting upload.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 3.939431 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 4.001312  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.942624 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 3.663777  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.941598 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 3.503759  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.940176 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 3.847359  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.940774 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 3.587716  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.940508 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 3.889457  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.941192 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 3.710948  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.946245 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 4.214440  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.944645 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 4.083396  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.944056 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 3.845081  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.946227 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 3.475434  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.947239 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 3.692235  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.946992 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 3.583977  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.948177 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 4.057424  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.950881 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 3.214953  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.947598 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 3.777477  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.947742 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 3.391724  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.948500 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 3.129686  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.948462 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 3.856991  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.949283 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 3.557257  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.949345 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 4.532489  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.947987 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 3.097089  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.946652 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 3.783085  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.946240 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 3.069760  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.944239 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 3.492442  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.943170 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 3.298757  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.942857 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 3.613906  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.939803 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 3.287341  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.939852 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 3.453622  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.937881 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 3.029814  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.939668 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 4.043387  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.937063 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 3.394979  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.935135 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 3.847568  [   64/89440]\n",
      "File /blue/gkalamangalam/jmark.ettinger/predictScalp/pytorchModels/model1.pth changed during upload, restarting upload.\n",
      "Test Error: \n",
      " Avg loss: 3.936355 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 3.471321  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.933940 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 3.178367  [   64/89440]\n",
      "File /blue/gkalamangalam/jmark.ettinger/predictScalp/pytorchModels/model1.pth changed during upload, restarting upload.\n",
      "Test Error: \n",
      " Avg loss: 3.935143 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 3.511522  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.933945 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 3.819813  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.934547 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 3.714861  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.934054 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 3.719458  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.933731 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 3.489854  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.935860 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 4.058842  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.935146 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 3.248099  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.933090 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 3.420091  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.933733 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 3.207806  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.933842 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 4.054407  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.934989 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 4.295186  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.935356 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 3.841422  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.933578 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 3.728724  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.932963 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 3.346114  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.932856 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 3.696055  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.930258 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 3.507963  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.929097 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 4.130877  [   64/89440]\n",
      "File /blue/gkalamangalam/jmark.ettinger/predictScalp/pytorchModels/model1.pth changed during upload, restarting upload.\n",
      "Test Error: \n",
      " Avg loss: 3.928683 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 3.353385  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.931270 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 4.382901  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.929354 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 3.458085  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.929858 \n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 3.720056  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.929137 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 4.607520  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.927739 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 3.843786  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.929457 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 3.325101  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.927742 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 3.769806  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.928865 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 3.716347  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.928388 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 3.454505  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.927504 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 3.296411  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.927666 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 3.871224  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.927097 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 3.428708  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.928925 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 3.890084  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.927061 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 4.283783  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.927209 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 4.075578  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.925999 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 3.694697  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.926062 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 3.523960  [   64/89440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 3.927152 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 3.179786  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.926916 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 3.551749  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.928550 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 3.316071  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.926388 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 3.785127  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.925567 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 3.531979  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.925649 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 3.799834  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.925434 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 3.578845  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.923419 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 3.945816  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.925492 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 3.344288  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.923548 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 4.018418  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.926050 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 3.396816  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.924115 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 4.110396  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.922964 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 4.313993  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.923407 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 3.778744  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.924661 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 3.050974  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.925544 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 3.730342  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.927922 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 3.101118  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.927680 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 3.844648  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.926949 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 4.166204  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.927243 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 4.040267  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.926441 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 3.210196  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.925458 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 4.267313  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.926722 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 4.707520  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.924596 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 3.294186  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.924105 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 3.576648  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.925846 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 4.066719  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.924959 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 3.502556  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.923553 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 3.214513  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.923538 \n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 3.312846  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.925222 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 3.583158  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.923694 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 4.000912  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.923356 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 3.236024  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.924295 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 3.484653  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.923488 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 3.986619  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.923856 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 3.982134  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.922992 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 3.406041  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.925349 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 3.845031  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.922988 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 3.289912  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.925017 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 3.777384  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.925582 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 3.110214  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.922884 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 3.330138  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.924315 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 3.679324  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.924466 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 3.604127  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.924292 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 3.324134  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.923635 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 3.955462  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.924191 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 3.793685  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.924329 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 3.216269  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.926805 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 3.371008  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.922979 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 3.041281  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.926266 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 3.365161  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.926088 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 3.419625  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.926287 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 3.688734  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.926436 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 3.632895  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.926812 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 3.597831  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.926805 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 3.842680  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.927243 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 3.226676  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.929560 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 3.185998  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.931050 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 3.639199  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.929248 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 4.073281  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.929614 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 3.563123  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.930775 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 3.434463  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.930885 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 3.472465  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.929518 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 3.677383  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.929730 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 3.307613  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.931765 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 3.287197  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.933518 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 3.440699  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.933122 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 4.057096  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.932012 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 3.874327  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.932985 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 3.903675  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.930634 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 3.722633  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.929331 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 3.896535  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.930444 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 3.710705  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.931653 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 3.497504  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.928287 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 3.077897  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.927676 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 3.134722  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.928103 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 3.533720  [   64/89440]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 3.927823 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 3.854582  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.926114 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 3.284026  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.927172 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 4.441832  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.927553 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 4.249127  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.928012 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 3.783826  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.928942 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 3.324861  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.926971 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 3.810966  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.928077 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 3.284823  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.925314 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 3.658512  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.926401 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 3.533756  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.923663 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 3.129740  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.924930 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 3.678374  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.922687 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 4.303399  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.922774 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 3.311680  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.921871 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 3.892675  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.922171 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 3.597831  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.920895 \n",
      "\n",
      "File /blue/gkalamangalam/jmark.ettinger/predictScalp/pytorchModels/model1.pth changed during upload, restarting upload.\n",
      "Saved a new best model!\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 3.793980  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.921843 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 3.266837  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.920873 \n",
      "\n",
      "Saved a new best model!\n",
      "File /blue/gkalamangalam/jmark.ettinger/predictScalp/pytorchModels/model1.pth changed during upload, restarting upload.\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 3.814750  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.921986 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 3.889620  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.920618 \n",
      "\n",
      "Saved a new best model!\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 3.231087  [   64/89440]\n",
      "File /blue/gkalamangalam/jmark.ettinger/predictScalp/pytorchModels/model1.pth changed during upload, restarting upload.\n",
      "Test Error: \n",
      " Avg loss: 3.921076 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 3.674231  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.921215 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 3.781301  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.920709 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 3.692093  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.922543 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 3.773972  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.921417 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 3.334131  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.924379 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 3.453361  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.921912 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 3.787764  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.921400 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 3.762174  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.921459 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 3.379655  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.923362 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 4.126050  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.921612 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 3.850740  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.920719 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 3.152395  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.924212 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 4.069587  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.922203 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 3.881602  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.922727 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 3.810578  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.925224 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 3.304519  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.924580 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 3.293379  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.924331 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 3.419975  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.923939 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 3.445979  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.921397 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 3.413260  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.921992 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 3.717177  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.924318 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 3.642144  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.923794 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 3.051836  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.923779 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 3.967981  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.922534 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 3.353248  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.922630 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 3.271610  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.921206 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 3.531970  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.923197 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 3.584809  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.922405 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 3.796159  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.922633 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 3.297461  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.920640 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 3.734476  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.921090 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 3.671577  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.923160 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 3.432090  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.921026 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 3.463895  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.920919 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 3.441762  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.921839 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 3.576504  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.921903 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 3.707521  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.920889 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 3.651803  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.921362 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 3.714714  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.921740 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 3.462820  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.922046 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 3.954090  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.922716 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 3.667809  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.922824 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 3.345257  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.924807 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 4.092422  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.924664 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 3.457806  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.925689 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 3.320901  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.924019 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 3.317011  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.926004 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 3.871176  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.926243 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 3.762240  [   64/89440]\n",
      "Test Error: \n",
      " Avg loss: 3.925293 \n",
      "\n",
      "Early stopping invoked....\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 7 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 7 operations synced, thanks for waiting!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/jettinger35/predictScalp/e/PRED-12/metadata\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "if optChoice == 'sgd':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "elif optChoice == 'adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "else:\n",
    "    optimizer = None\n",
    "    print('no optimizer chosen...')\n",
    "    \n",
    "early_stopper = EarlyStopper(patience=patience, min_delta=min_delta)\n",
    "\n",
    "run = neptune.init_run(\n",
    "    project=neptuneProject,\n",
    "    api_token=api_token,  \n",
    "    capture_hardware_metrics=True,\n",
    "    capture_stderr=True,\n",
    "    capture_stdout=True,\n",
    ")\n",
    "\n",
    "PARAMS = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": learningRate,\n",
    "    \"optimizer\": optChoice,\n",
    "    \"patience\": patience,\n",
    "    \"min_delta\": min_delta,\n",
    "    \"subsampleFreq\": subsampleFreq,\n",
    "    \"secondsInWindow\": secondsInWindow,\n",
    "    \"nperseg\": nperseg,\n",
    "    \"noverlap\": noverlap,\n",
    "    \"window\": stringify_unsupported(window)\n",
    "}\n",
    "run[\"parameters\"] = PARAMS\n",
    "\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loss = train(trainDataLoader, model, loss_fn, optimizer)\n",
    "    test_loss = test(validDataLoader, model, loss_fn)\n",
    "    \n",
    "    if test_loss < bestTestLoss:\n",
    "        bestTestLoss = test_loss\n",
    "        torch.save(model, modelPath)\n",
    "        run[\"model_best\"].upload(modelPath)\n",
    "        print(\"\\nSaved a new best model!\\n\")\n",
    "        \n",
    "    run[\"train/loss\"].append(train_loss)\n",
    "    run[\"test/loss\"].append(test_loss)\n",
    "    \n",
    "    if early_stopper.early_stop(test_loss):   \n",
    "        print(\"Early stopping invoked....\")\n",
    "        break\n",
    "        \n",
    "run.stop()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49094c64",
   "metadata": {},
   "source": [
    "# Plot results of fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a858773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT PREDICTION VERSUS TRUTH\n",
    "\n",
    "trainPlotFlag = False\n",
    "    \n",
    "if trainPlotFlag:\n",
    "    x = trainXTensor\n",
    "    y = y_trainRTheta\n",
    "    trainTitle = 'train'\n",
    "else:\n",
    "    x = validXTensor\n",
    "    y = y_validRTheta\n",
    "    trainTitle = 'valididation'\n",
    "    \n",
    "\n",
    "x = validXTensor.to(device)\n",
    "freqPredict = model(x).cpu().detach().numpy()\n",
    "\n",
    "_, yPred = realSTFTtoTimeSeries(freqPredict)\n",
    "_, yTrue = realSTFTtoTimeSeries(y)\n",
    "\n",
    "lossTemp = loss_fn(torch.tensor(yPred), torch.tensor(yTrue)).item()\n",
    "title = 'PyTorch: ' + trainTitle + ' (mse: %s)' % str(lossTemp)\n",
    "plt.figure()\n",
    "plt.plot(yPred, label='predict')\n",
    "plt.plot(yTrue, label='true')\n",
    "plt.legend()\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3021d04b",
   "metadata": {},
   "source": [
    "# SCRATCH BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e270d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "import neptune\n",
    "\n",
    "# Create a Neptune run object\n",
    "run = neptune.init_run(\n",
    "    project='jettinger35/test',\n",
    "    api_token=api_token,  \n",
    ")\n",
    "\n",
    "# Track metadata and hyperparameters by assigning them to the run\n",
    "run[\"JIRA\"] = \"NPT-952\"\n",
    "run[\"algorithm\"] = \"ConvNet\"\n",
    "\n",
    "PARAMS = {\n",
    "    \"batch_size\": 64,\n",
    "    \"dropout\": 0.2,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer\": \"Adam\",\n",
    "}\n",
    "run[\"parameters\"] = PARAMS\n",
    "\n",
    "# Track the training process by logging your training metrics\n",
    "for epoch in range(10):\n",
    "    run[\"train/accuracy\"].append(epoch * 0.6)  \n",
    "    run[\"train/loss\"].append(epoch * 0.4)\n",
    "\n",
    "# Record the final results\n",
    "run[\"f1_score\"] = 0.66\n",
    "\n",
    "# Stop the connection and synchronize the data with the Neptune servers\n",
    "run.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27671e06",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-1.13",
   "language": "python",
   "name": "pytorch-1.13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "li7HX-beZFGg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import neptune\n",
    "from neptune_pytorch import NeptuneLogger\n",
    "from neptune.utils import stringify_unsupported\n",
    "\n",
    "import datetime\n",
    "import pytz\n",
    "timeZone = pytz.timezone('America/Los_Angeles')\n",
    "from operator import itemgetter\n",
    "\n",
    "from eegUtils import *\n",
    "import torchModels\n",
    "from myUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "li7HX-beZFGg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(torchModels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGXT2d4lB0n5"
   },
   "source": [
    "# Read the data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z78SjRnaX9bF",
    "outputId": "439db100-5aa5-4b49-850d-6b0cf3135775",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been moved to GPU\n",
      "(19, 1100367)\n"
     ]
    }
   ],
   "source": [
    "dataMultiply = 10**5\n",
    "sFreq = 256\n",
    "arrayInCompressedFile = 'arr_7'\n",
    "\n",
    "data = np.load('/blue/gkalamangalam/jmark.ettinger/eegCompress/processedData/elimPeaksSVD001.npz')[arrayInCompressedFile]\n",
    "nChannel, nSample = data.shape\n",
    "data = (data * dataMultiply).astype('float32')\n",
    "dataTensor = torch.tensor(data)\n",
    "if torch.cuda.is_available():\n",
    "    dataTensor = dataTensor.to('cuda')\n",
    "    print(\"Data has been moved to GPU\")\n",
    "else:\n",
    "    print(\"Data is on CPU\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = '/blue/gkalamangalam/jmark.ettinger/eegCompress/processedData/kmeansModels/kmeansModel_001_block7_1stack.npz'\n",
    "npzfile = np.load(path)\n",
    "centroids = npzfile['arr_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byjCmPBeZFGo"
   },
   "source": [
    "# Define Model, Optimizer, DataSet and Optionally Load All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OGgAiuo8ZFGo",
    "outputId": "f1a3ae86-dfe2-40c6-f3b5-364f07b648bb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 9.619MB\n",
      "conv1dKmeans(\n",
      "  (myNet): Sequential(\n",
      "    (0): Conv1d(19, 50, kernel_size=(3,), stride=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv1d(50, 50, kernel_size=(3,), stride=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv1d(50, 50, kernel_size=(3,), stride=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=700, out_features=19, bias=True)\n",
      "  )\n",
      ")\n",
      "Model has been moved to GPU\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "modelType = 'kmeans'\n",
    "loadBool = 0\n",
    "#modelPath = '/content/drive/MyDrive/NeuroResearch/Data/eegCompress/models/savedModel_03-28 19:04_-0.034.pt'\n",
    "numSampleInput = 20\n",
    "numSampleOutput = 1\n",
    "\n",
    "\n",
    "initDict = {'kmeansInit': centroids, 'dataTensor': dataTensor, 'numSampleInput':numSampleInput}\n",
    "\n",
    "model, dataset, loss_function = torchModels.makeModel(modelType, initDict)\n",
    "# modelType, nChannel, numSampleInput, numSampleOutput, dataTensor\n",
    "\n",
    "sizeOfModel = modelSize(model)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.0)\n",
    "#optimizer = torch.optim.Adam(model.parameters())#, lr = 1e-1, weight_decay = 1e-8)\n",
    "\n",
    "if loadBool:\n",
    "    model, optimizer, totalEpoch, loss = loadModel(modelPath, model, optimizer, trainBool=True)\n",
    "    print(\"Model has been loaded: \" + modelPath)\n",
    "else:\n",
    "    totalEpoch = 0\n",
    "\n",
    "print(model)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda')\n",
    "    print(\"Model has been moved to GPU\")\n",
    "else:\n",
    "    print(\"Model is on CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WWeZ4PRYCGWR",
    "outputId": "7b4deb16-8cc9-4416-a7bb-a7d0ce4ba240",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/jettinger35/eegCompress/e/EEG-237\n"
     ]
    }
   ],
   "source": [
    "logFlag = True\n",
    "\n",
    "if logFlag:\n",
    "  run = neptune.init_run(\n",
    "      project=\"jettinger35/eegCompress\",\n",
    "      api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzMjFlMzY2MS1iOWZiLTRmZWEtOGMwNy0zOTVkMTljOGVjYTMifQ==\",\n",
    "      #with_id=\"EEG-116\"\n",
    "      )\n",
    "\n",
    "  npt_logger = NeptuneLogger(\n",
    "      run=run,\n",
    "      model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvj5MCOA1ASz"
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3oawEnUlXEOO",
    "outputId": "93ffa385-10e2-4b73-9270-a42c5b0ae4c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-15 11:33: Epoch: 0\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batchSize = 32\n",
    "numRandomPlot = 2\n",
    "secondsToPlot = 5\n",
    "saveEveryNEpochs = 0 # 0 for no saving\n",
    "\n",
    "# initialize\n",
    "breakFlag = False\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=False, sampler=None)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    counter = 0\n",
    "\n",
    "    myPrint(\"Epoch: \" + str(totalEpoch))\n",
    "    if breakFlag:\n",
    "        myPrint(\"Break!\")\n",
    "        break\n",
    "    for (thisBlock, label) in loader:\n",
    "        counter += 1\n",
    "        prediction = model(thisBlock)\n",
    "        if np.any(np.isnan(prediction.detach().cpu().numpy())):\n",
    "            myPrint(\"NaN detected.  Counter: \" + str(counter))\n",
    "            breakFlag = True\n",
    "            break\n",
    "        loss = loss_function(prediction, label).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        max_norm = 1.0\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        if logFlag:\n",
    "            run[npt_logger.base_namespace][\"train/log_loss\"].append(np.log(loss.item()))\n",
    "\n",
    "    if logFlag:\n",
    "      # plot random locations for original and predicted for comparison\n",
    "      for i in range(numRandomPlot):\n",
    "            startPlot = random.randint(0, nSample - (secondsToPlot * sFreq))\n",
    "            fig, original, predicted = timeSeriesCompare(model, startPlot, secondsToPlot, sFreq, data, numSampleInput)\n",
    "            plt.title(\"Epoch, Start, Blocks: \" + str((totalEpoch, startPlot, secondsToPlot)))\n",
    "            run[\"fig\"].append(fig)\n",
    "            plt.close()\n",
    "\n",
    "    totalEpoch += 1\n",
    "\n",
    "    print(\"Predicting and generating sampler...\")\n",
    "    sampler, predicted, residualMeasure = samplerMake(model, numSampleInput, data)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=False, sampler=sampler)\n",
    "\n",
    "    if saveEveryNEpochs > 0 and (epoch + 1) % saveEveryNEpochs == 0:\n",
    "        saveModel(model, optimizer, totalEpoch, loss, predicted)\n",
    "\n",
    "myPrint(\"Finished training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aPNgp4gW_5W"
   },
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_LLbQ5rDW-tA",
    "outputId": "4c4ab8b4-a7ae-4b24-ee5d-b88d8ef9aeec"
   },
   "outputs": [],
   "source": [
    "sampler, predicted, residualMeasure = samplerMake(model, numSampleInput, data)\n",
    "saveModel(model, optimizer, totalEpoch, loss, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjGv6LJS0u06"
   },
   "source": [
    "# Compare the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "8MX4N__GVAWa",
    "outputId": "f50ef3ad-bc9d-4998-f797-3cfd7e821658"
   },
   "outputs": [],
   "source": [
    "plotBool = 1\n",
    "\n",
    "if plotBool:\n",
    "  startPlot = 1000\n",
    "  secondsToPlot = 5\n",
    "  channel = 0\n",
    "  plotOption = 'both'\n",
    "\n",
    "  fig, original, predicted64 = timeSeriesCompare(model, startPlot, secondsToPlot, sFreq, data, numSampleInput, channel, plotOption)\n",
    "  plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiajJ-Hq1f6m"
   },
   "source": [
    "# Save original and predicted data for local graphical comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8PRONaU1e13"
   },
   "outputs": [],
   "source": [
    "predicted = predictEEG(model, None, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kap82C5COj2s"
   },
   "outputs": [],
   "source": [
    "path = '/content/drive/MyDrive/NeuroResearch/Data/eegCompress/processedData/origAndPredictedSVD001_block7.npz'\n",
    "dataToSaveList = [data, predicted]\n",
    "np.savez_compressed(path, *dataToSaveList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxVAcHWqPbt8"
   },
   "source": [
    "# Show network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dTYLWBOCfp9W",
    "outputId": "61763a4e-86c1-429e-820e-e77cc1a7592a"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print((param.shape, param.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhhtZuYDBCP6"
   },
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT ONNX FOR VISUALIZATION IN NETRON APP\n",
    "\n",
    "visualizationPath = '/content/drive/MyDrive/NeuroResearch/Data/eegCompress/models/model.onnx'\n",
    "dataset = datasetMake(torch.tensor(data[:,100:200]), model.numSampleInput, model.typeCode)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None)\n",
    "batch, label = next(loader.__iter__())\n",
    "yhat = model(batch)\n",
    "\n",
    "torch.onnx.export(model, batch, f=visualizationPath)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myPytorch",
   "language": "python",
   "name": "mypytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

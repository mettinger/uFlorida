{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "li7HX-beZFGg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import neptune\n",
    "from neptune_pytorch import NeptuneLogger\n",
    "from neptune.utils import stringify_unsupported\n",
    "\n",
    "import datetime\n",
    "import pytz\n",
    "timeZone = pytz.timezone('America/Los_Angeles')\n",
    "from operator import itemgetter\n",
    "\n",
    "#from eegUtils import *\n",
    "import torchModels\n",
    "import eegUtils\n",
    "from myUtils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "li7HX-beZFGg",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'eegUtils' from '/home/jmark.ettinger/github/uFlorida/eegCompress/eegUtils.py'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Communication with Neptune restored!\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "importlib.reload(torchModels)\n",
    "importlib.reload(eegUtils)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGXT2d4lB0n5"
   },
   "source": [
    "# Read the data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z78SjRnaX9bF",
    "outputId": "439db100-5aa5-4b49-850d-6b0cf3135775",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been moved to GPU\n",
      "(19, 1100367)\n"
     ]
    }
   ],
   "source": [
    "dataMultiply = 10**5\n",
    "sFreq = 256\n",
    "arrayInCompressedFile = 'arr_7'\n",
    "\n",
    "data = np.load('/blue/gkalamangalam/jmark.ettinger/eegCompress/processedData/elimPeaksSVD001.npz')[arrayInCompressedFile]\n",
    "nChannel, nSample = data.shape\n",
    "data = (data * dataMultiply).astype('float32')\n",
    "dataTensor = torch.tensor(data)\n",
    "if torch.cuda.is_available():\n",
    "    dataTensor = dataTensor.to('cuda')\n",
    "    print(\"Data has been moved to GPU\")\n",
    "else:\n",
    "    print(\"Data is on CPU\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = '/blue/gkalamangalam/jmark.ettinger/eegCompress/processedData/kmeansModels/kmeansModel_001_block7_1stack.npz'\n",
    "npzfile = np.load(path)\n",
    "centroids = npzfile['arr_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byjCmPBeZFGo"
   },
   "source": [
    "# Define Model, Optimizer, DataSet and Optionally Load All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OGgAiuo8ZFGo",
    "outputId": "f1a3ae86-dfe2-40c6-f3b5-364f07b648bb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 9.619MB\n",
      "Model has been loaded: /blue/gkalamangalam/jmark.ettinger/eegCompress/models/savedModel_04-16 00:52_-0.023.pt\n",
      "conv1dKmeans(\n",
      "  (myNet): Sequential(\n",
      "    (0): Conv1d(19, 50, kernel_size=(3,), stride=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv1d(50, 50, kernel_size=(3,), stride=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv1d(50, 50, kernel_size=(3,), stride=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=700, out_features=19, bias=True)\n",
      "  )\n",
      ")\n",
      "Model has been moved to GPU\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "modelType = 'kmeans'\n",
    "loadBool = 1\n",
    "modelPath = '/blue/gkalamangalam/jmark.ettinger/eegCompress/models/savedModel_04-16 00:52_-0.023.pt'\n",
    "numSampleInput = 20\n",
    "numSampleOutput = 1\n",
    "\n",
    "\n",
    "initDict = {'kmeansInit': centroids, 'dataTensor': dataTensor, 'numSampleInput':numSampleInput}\n",
    "\n",
    "model, dataset, loss_function = torchModels.makeModel(modelType, initDict)\n",
    "# modelType, nChannel, numSampleInput, numSampleOutput, dataTensor\n",
    "\n",
    "sizeOfModel = eegUtils.modelSize(model)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.0)\n",
    "#optimizer = torch.optim.Adam(model.parameters())#, lr = 1e-1, weight_decay = 1e-8)\n",
    "\n",
    "if loadBool:\n",
    "    model, optimizer, totalEpoch, loss = eegUtils.loadModel(modelPath, model, optimizer, trainBool=True)\n",
    "    print(\"Model has been loaded: \" + modelPath)\n",
    "else:\n",
    "    totalEpoch = 0\n",
    "\n",
    "print(model)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda')\n",
    "    print(\"Model has been moved to GPU\")\n",
    "else:\n",
    "    print(\"Model is on CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WWeZ4PRYCGWR",
    "outputId": "7b4deb16-8cc9-4416-a7bb-a7d0ce4ba240",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[neptune] [warning] NeptuneWarning: By default, these monitoring options are disabled in interactive sessions: 'capture_stdout', 'capture_stderr', 'capture_traceback', 'capture_hardware_metrics'. You can set them to 'True' when initializing the run and the monitoring will continue until you call run.stop() or the kernel stops. NOTE: To track the source files, pass their paths to the 'source_code' argument. For help, see: https://docs.neptune.ai/logging/source_code/\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[neptune] [info   ] Neptune initialized. Open in the app: https://app.neptune.ai/jettinger35/eegCompress/e/EEG-240\n"
     ]
    }
   ],
   "source": [
    "logFlag = True\n",
    "\n",
    "if logFlag:\n",
    "  run = neptune.init_run(\n",
    "      project=\"jettinger35/eegCompress\",\n",
    "      api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzMjFlMzY2MS1iOWZiLTRmZWEtOGMwNy0zOTVkMTljOGVjYTMifQ==\",\n",
    "      #with_id=\"EEG-116\"\n",
    "      )\n",
    "\n",
    "  npt_logger = NeptuneLogger(\n",
    "      run=run,\n",
    "      model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvj5MCOA1ASz"
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3oawEnUlXEOO",
    "outputId": "93ffa385-10e2-4b73-9270-a42c5b0ae4c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-15 17:34: Epoch: 50\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.281986108637954\n",
      "04-15 17:38: Epoch: 51\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.323059277707024\n",
      "04-15 17:42: Epoch: 52\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.299448290937825\n",
      "04-15 17:46: Epoch: 53\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.297957247025089\n",
      "04-15 17:50: Epoch: 54\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.291096642613411\n",
      "04-15 17:54: Epoch: 55\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.304183226863021\n",
      "04-15 17:58: Epoch: 56\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: ConnectionError\n",
      "[neptune] [info   ] Communication with Neptune restored!\n",
      "[neptune] [info   ] Communication with Neptune restored!\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.299037627091533\n",
      "04-15 18:02: Epoch: 57\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.307055258633275\n",
      "04-15 18:06: Epoch: 58\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.30426829347485\n",
      "04-15 18:10: Epoch: 59\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.308396182366108\n",
      "Model has been saved: savedModel_04-15 18:14_-0.022.pt\n",
      "04-15 18:14: Epoch: 60\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.309106847565426\n",
      "04-15 18:18: Epoch: 61\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.313325461196271\n",
      "04-15 18:22: Epoch: 62\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: ConnectionError\n",
      "[neptune] [info   ] Communication with Neptune restored!\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.312640482265698\n",
      "04-15 18:26: Epoch: 63\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.307262270074142\n",
      "04-15 18:30: Epoch: 64\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.30810675044593\n",
      "04-15 18:34: Epoch: 65\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "[neptune] [info   ] Communication with Neptune restored!\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.311450297503095\n",
      "04-15 18:38: Epoch: 66\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.316580482317429\n",
      "04-15 18:42: Epoch: 67\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.29965489161642\n",
      "04-15 18:46: Epoch: 68\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.297658359631896\n",
      "04-15 18:50: Epoch: 69\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.30254762845212\n",
      "Model has been saved: savedModel_04-15 18:54_-0.025.pt\n",
      "04-15 18:54: Epoch: 70\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "[neptune] [info   ] Communication with Neptune restored!\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.29560328245555\n",
      "04-15 18:58: Epoch: 71\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.284274948662834\n",
      "04-15 19:02: Epoch: 72\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.30627194311666\n",
      "04-15 19:06: Epoch: 73\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.314429209616623\n",
      "04-15 19:10: Epoch: 74\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.308824091561531\n",
      "04-15 19:14: Epoch: 75\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: ConnectionError\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "[neptune] [info   ] Communication with Neptune restored!\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.32429641602855\n",
      "04-15 19:18: Epoch: 76\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.313228704917588\n",
      "04-15 19:22: Epoch: 77\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.31267571625741\n",
      "04-15 19:25: Epoch: 78\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.304838658947693\n",
      "04-15 19:29: Epoch: 79\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.295532170487078\n",
      "Model has been saved: savedModel_04-15 19:33_-0.025.pt\n",
      "04-15 19:33: Epoch: 80\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.312815271710095\n",
      "04-15 19:37: Epoch: 81\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.299823978621708\n",
      "04-15 19:41: Epoch: 82\n",
      "[neptune] [info   ] Communication with Neptune restored!\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: ConnectionError\n",
      "[neptune] [info   ] Communication with Neptune restored!\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.294092594792968\n",
      "04-15 19:45: Epoch: 83\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.298210271193009\n",
      "04-15 19:49: Epoch: 84\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.302115275377506\n",
      "04-15 19:53: Epoch: 85\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.299250931724123\n",
      "04-15 19:57: Epoch: 86\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.29683060169612\n",
      "04-15 20:01: Epoch: 87\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.288321779158554\n",
      "04-15 20:05: Epoch: 88\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: ConnectionError\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "[neptune] [info   ] Communication with Neptune restored!\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.306368409803039\n",
      "04-15 20:09: Epoch: 89\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.280195432077898\n",
      "Model has been saved: savedModel_04-15 20:13_-0.024.pt\n",
      "04-15 20:13: Epoch: 90\n",
      "[neptune] [info   ] Communication with Neptune restored!\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.304085984433952\n",
      "04-15 20:17: Epoch: 91\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.299733648664857\n",
      "04-15 20:21: Epoch: 92\n",
      "Predicting and generating sampler...\n",
      "Residual measure: 11.286934588104486\n",
      "04-15 20:26: Epoch: 93\n"
     ]
    }
   ],
   "source": [
    "epochs = 110\n",
    "batchSize = 32\n",
    "numRandomPlot = 2\n",
    "samplesToPlot = 5 * sFreq\n",
    "saveEveryNEpochs = 10 # 0 for no saving\n",
    "\n",
    "# initialize\n",
    "breakFlag = False\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=False, sampler=None)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    counter = 0\n",
    "\n",
    "    myPrint(\"Epoch: \" + str(totalEpoch))\n",
    "    if breakFlag:\n",
    "        myPrint(\"Break!\")\n",
    "        break\n",
    "    for (thisBlock, label) in loader:\n",
    "        counter += 1\n",
    "        prediction = model(thisBlock)\n",
    "        if np.any(np.isnan(prediction.detach().cpu().numpy())):\n",
    "            myPrint(\"NaN detected.  Counter: \" + str(counter))\n",
    "            breakFlag = True\n",
    "            break\n",
    "        loss = loss_function(prediction, label).mean()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        max_norm = 1.0\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        if logFlag:\n",
    "            run[npt_logger.base_namespace][\"train/log_loss\"].append(np.log(loss.item()))\n",
    "\n",
    "    print(\"Predicting and generating sampler...\")\n",
    "    sampler, predicted, residualMeasure = eegUtils.samplerMake(model, numSampleInput, data)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=False, sampler=sampler)\n",
    "    \n",
    "    if logFlag:\n",
    "      # plot random locations for original and predicted for comparison\n",
    "      for i in range(numRandomPlot):\n",
    "            startPlot = random.randint(0, nSample - (secondsToPlot * sFreq))\n",
    "            #fig, original, predicted = eegUtils.timeSeriesCompare(model, startPlot, secondsToPlot, sFreq, data, numSampleInput)\n",
    "            fig = timeSeriesCompare(original, predicted, startPlot, samplesToPLot, channel = 0, plotOption=\"both\")\n",
    "            plt.title(\"Epoch, Start, Blocks: \" + str((totalEpoch, startPlot, secondsToPlot)))\n",
    "            run[\"fig\"].append(fig)\n",
    "            plt.close()\n",
    "\n",
    "    totalEpoch += 1\n",
    "\n",
    "    if saveEveryNEpochs > 0 and (epoch + 1) % saveEveryNEpochs == 0:\n",
    "        eegUtils.saveModel(model, optimizer, totalEpoch, loss, predicted)\n",
    "\n",
    "myPrint(\"Finished training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aPNgp4gW_5W"
   },
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_LLbQ5rDW-tA",
    "outputId": "4c4ab8b4-a7ae-4b24-ee5d-b88d8ef9aeec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual measure: 11.298229327719463\n",
      "Model has been saved: savedModel_04-15 16:49_-0.025.pt\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: ConnectionError\n"
     ]
    }
   ],
   "source": [
    "sampler, predicted, residualMeasure = eegUtils.samplerMake(model, numSampleInput, data)\n",
    "eegUtils.saveModel(model, optimizer, totalEpoch, loss, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjGv6LJS0u06"
   },
   "source": [
    "# Compare the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "8MX4N__GVAWa",
    "outputId": "f50ef3ad-bc9d-4998-f797-3cfd7e821658"
   },
   "outputs": [],
   "source": [
    "plotBool = 1\n",
    "\n",
    "if plotBool:\n",
    "    startPlot = 1000\n",
    "    samplesToPlot = 5 * sFreq\n",
    "    channel = 0\n",
    "    plotOption = 'both'\n",
    "    fig = timeSeriesCompare(data, predicted, startPlot, samplesToPLot, channel = 0, plotOption=\"both\")\n",
    "    #fig, original, predicted64 = timeSeriesCompare(model, startPlot, secondsToPlot, sFreq, data, numSampleInput, channel, plotOption)\n",
    "    plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiajJ-Hq1f6m"
   },
   "source": [
    "# Save original and predicted data for local graphical comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8PRONaU1e13"
   },
   "outputs": [],
   "source": [
    "predicted = predictEEG(model, None, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kap82C5COj2s"
   },
   "outputs": [],
   "source": [
    "path = '/content/drive/MyDrive/NeuroResearch/Data/eegCompress/processedData/origAndPredictedSVD001_block7.npz'\n",
    "dataToSaveList = [data, predicted]\n",
    "np.savez_compressed(path, *dataToSaveList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxVAcHWqPbt8"
   },
   "source": [
    "# Show network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dTYLWBOCfp9W",
    "outputId": "61763a4e-86c1-429e-820e-e77cc1a7592a"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print((param.shape, param.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhhtZuYDBCP6"
   },
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT ONNX FOR VISUALIZATION IN NETRON APP\n",
    "\n",
    "visualizationPath = '/content/drive/MyDrive/NeuroResearch/Data/eegCompress/models/model.onnx'\n",
    "dataset = datasetMake(torch.tensor(data[:,100:200]), model.numSampleInput, model.typeCode)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None)\n",
    "batch, label = next(loader.__iter__())\n",
    "yhat = model(batch)\n",
    "\n",
    "torch.onnx.export(model, batch, f=visualizationPath)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myPytorch",
   "language": "python",
   "name": "mypytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

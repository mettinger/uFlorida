{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "li7HX-beZFGg",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torchModels' from '/home/jmark.ettinger/github/uFlorida/eegCompress/torchModels.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import neptune\n",
    "from neptune_pytorch import NeptuneLogger\n",
    "from neptune.utils import stringify_unsupported\n",
    "\n",
    "import datetime\n",
    "import pytz\n",
    "timeZone = pytz.timezone('America/Los_Angeles')\n",
    "from operator import itemgetter\n",
    "\n",
    "from eegUtils import *\n",
    "import torchModels\n",
    "from myUtils import *\n",
    "\n",
    "import importlib\n",
    "importlib.reload(torchModels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGXT2d4lB0n5"
   },
   "source": [
    "# Read the data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z78SjRnaX9bF",
    "outputId": "439db100-5aa5-4b49-850d-6b0cf3135775",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been moved to GPU\n",
      "(19, 1100367)\n"
     ]
    }
   ],
   "source": [
    "dataMultiply = 10**5\n",
    "sFreq = 256\n",
    "arrayInCompressedFile = 'arr_7'\n",
    "\n",
    "data = np.load('/blue/gkalamangalam/jmark.ettinger/eegCompress/processedData/elimPeaksSVD001.npz')[arrayInCompressedFile]\n",
    "nChannel, nSample = data.shape\n",
    "data = (data * dataMultiply).astype('float32')\n",
    "dataTensor = torch.tensor(data)\n",
    "if torch.cuda.is_available():\n",
    "  dataTensor = dataTensor.to('cuda')\n",
    "  print(\"Data has been moved to GPU\")\n",
    "else:\n",
    "  print(\"Data is on CPU\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = '/blue/gkalamangalam/jmark.ettinger/eegCompress/processedData/kmeansModels/kmeansModel_001_block7_1stack.npz'\n",
    "npzfile = np.load(path)\n",
    "centroids = npzfile['arr_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byjCmPBeZFGo"
   },
   "source": [
    "# Define Model, Optimizer, DataSet and Optionally Load All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OGgAiuo8ZFGo",
    "outputId": "f1a3ae86-dfe2-40c6-f3b5-364f07b648bb",
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "conv1dKmeans.__init__() takes 2 positional arguments but 4 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m numSampleInput = \u001b[32m20\u001b[39m\n\u001b[32m      6\u001b[39m numSampleOutput = \u001b[32m1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m model, dataset, loss_function = \u001b[43mtorchModels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmakeModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodelType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnChannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumSampleInput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumSampleOutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataTensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m sizeOfModel = modelSize(model)\n\u001b[32m     11\u001b[39m optimizer = torch.optim.SGD(model.parameters(), lr=\u001b[32m0.001\u001b[39m, momentum=\u001b[32m0.0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/github/uFlorida/eegCompress/torchModels.py:30\u001b[39m, in \u001b[36mmakeModel\u001b[39m\u001b[34m(modelType, nChannel, numSampleInput, numSampleOutput, dataTensor)\u001b[39m\n\u001b[32m     27\u001b[39m nCentroids = \u001b[32m2\u001b[39m**\u001b[32m16\u001b[39m\n\u001b[32m     28\u001b[39m embeddingInit = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m model = \u001b[43mconv1dKmeans\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnChannel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnCentroids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membeddingInit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m dataset = datasetConv1dKmeans(dataTensor, numSampleInput)\n\u001b[32m     32\u001b[39m lossFunction = model.lossFunction\n",
      "\u001b[31mTypeError\u001b[39m: conv1dKmeans.__init__() takes 2 positional arguments but 4 were given"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "modelType = 'kmeans'\n",
    "loadBool = 0\n",
    "#modelPath = '/content/drive/MyDrive/NeuroResearch/Data/eegCompress/models/savedModel_03-28 19:04_-0.034.pt'\n",
    "numSampleInput = 20\n",
    "numSampleOutput = 1\n",
    "\n",
    "initDict = {'kmeansInit': centroids, 'dataTensor': dataTensor, 'numSampleInput':numSampleInput}\n",
    "model, dataset, loss_function = torchModels.makeModel(initDict)\n",
    "# modelType, nChannel, numSampleInput, numSampleOutput, dataTensor\n",
    "\n",
    "sizeOfModel = modelSize(model)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.0)\n",
    "#optimizer = torch.optim.Adam(model.parameters())#, lr = 1e-1, weight_decay = 1e-8)\n",
    "\n",
    "if loadBool:\n",
    "  model, optimizer, totalEpoch, loss = loadModel(modelPath, model, optimizer, trainBool=True)\n",
    "  print(\"Model has been loaded: \" + modelPath)\n",
    "else:\n",
    "  totalEpoch = 0\n",
    "\n",
    "print(model)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  model.to('cuda')\n",
    "  print(\"Model has been moved to GPU\")\n",
    "else:\n",
    "  print(\"Model is on CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WWeZ4PRYCGWR",
    "outputId": "7b4deb16-8cc9-4416-a7bb-a7d0ce4ba240",
    "tags": []
   },
   "outputs": [],
   "source": [
    "logFlag = True\n",
    "\n",
    "if logFlag:\n",
    "  run = neptune.init_run(\n",
    "      project=\"jettinger35/eegCompress\",\n",
    "      api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzMjFlMzY2MS1iOWZiLTRmZWEtOGMwNy0zOTVkMTljOGVjYTMifQ==\",\n",
    "      #with_id=\"EEG-116\"\n",
    "      )\n",
    "\n",
    "  npt_logger = NeptuneLogger(\n",
    "      run=run,\n",
    "      model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvj5MCOA1ASz"
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3oawEnUlXEOO",
    "outputId": "93ffa385-10e2-4b73-9270-a42c5b0ae4c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-13 10:38: Epoch: 0\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: ConnectionError\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "[neptune] [info   ] Communication with Neptune restored!\n",
      "[neptune] [info   ] Communication with Neptune restored!\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: ConnectionError\n",
      "[neptune] [info   ] Communication with Neptune restored!\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "[neptune] [info   ] Communication with Neptune restored!\n",
      "[neptune] [warning] Experiencing connection interruptions. Will try to reestablish communication with Neptune. Internal exception was: RequestsFutureAdapterTimeout\n",
      "[neptune] [info   ] Communication with Neptune restored!\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batchSize = 32\n",
    "numRandomPlot = 2\n",
    "secondsToPlot = 5\n",
    "saveEveryNEpochs = 0 # 0 for no saving\n",
    "\n",
    "# initialize\n",
    "breakFlag = False\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=False, sampler=None)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    counter = 0\n",
    "\n",
    "    myPrint(\"Epoch: \" + str(totalEpoch))\n",
    "    if breakFlag:\n",
    "        myPrint(\"Break!\")\n",
    "        break\n",
    "    for (thisBlock, label) in loader:\n",
    "        counter += 1\n",
    "        prediction = model(thisBlock)\n",
    "        if np.any(np.isnan(prediction.detach().cpu().numpy())):\n",
    "            myPrint(\"NaN detected.  Counter: \" + str(counter))\n",
    "            breakFlag = True\n",
    "            break\n",
    "        loss = loss_function(prediction, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        max_norm = 1.0\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        if logFlag:\n",
    "            run[npt_logger.base_namespace][\"train/log_loss\"].append(np.log(loss.item()))\n",
    "\n",
    "    if logFlag:\n",
    "      # plot random locations for original and predicted for comparison\n",
    "      for i in range(numRandomPlot):\n",
    "\n",
    "          startPlot = random.randint(0, nSample - (secondsToPlot * sFreq))\n",
    "          fig, original, predicted = timeSeriesCompare(model, startPlot, secondsToPlot, sFreq, data, numSampleInput)\n",
    "          plt.title(\"Epoch, Start, Blocks: \" + str((totalEpoch, startPlot, secondsToPlot)))\n",
    "          run[\"fig\"].append(fig)\n",
    "          plt.close()\n",
    "\n",
    "    totalEpoch += 1\n",
    "\n",
    "    print(\"Predicting and generating sampler...\")\n",
    "    sampler, predicted, residualMeasure = samplerMake(model, numSampleInput, data)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=False, sampler=sampler)\n",
    "\n",
    "    if saveEveryNEpochs > 0 and (epoch + 1) % saveEveryNEpochs == 0:\n",
    "      saveModel(model, optimizer, totalEpoch, loss, predicted)\n",
    "\n",
    "myPrint(\"Finished training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aPNgp4gW_5W"
   },
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_LLbQ5rDW-tA",
    "outputId": "4c4ab8b4-a7ae-4b24-ee5d-b88d8ef9aeec"
   },
   "outputs": [],
   "source": [
    "sampler, predicted, residualMeasure = samplerMake(model, numSampleInput, data)\n",
    "saveModel(model, optimizer, totalEpoch, loss, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjGv6LJS0u06"
   },
   "source": [
    "# Compare the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "8MX4N__GVAWa",
    "outputId": "f50ef3ad-bc9d-4998-f797-3cfd7e821658"
   },
   "outputs": [],
   "source": [
    "plotBool = 1\n",
    "\n",
    "if plotBool:\n",
    "  startPlot = 1000\n",
    "  secondsToPlot = 5\n",
    "  channel = 0\n",
    "  plotOption = 'both'\n",
    "\n",
    "  fig, original, predicted64 = timeSeriesCompare(model, startPlot, secondsToPlot, sFreq, data, numSampleInput, channel, plotOption)\n",
    "  plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiajJ-Hq1f6m"
   },
   "source": [
    "# Save original and predicted data for local graphical comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8PRONaU1e13"
   },
   "outputs": [],
   "source": [
    "predicted = predictEEG(model, None, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kap82C5COj2s"
   },
   "outputs": [],
   "source": [
    "path = '/content/drive/MyDrive/NeuroResearch/Data/eegCompress/processedData/origAndPredictedSVD001_block7.npz'\n",
    "dataToSaveList = [data, predicted]\n",
    "np.savez_compressed(path, *dataToSaveList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxVAcHWqPbt8"
   },
   "source": [
    "# Show network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dTYLWBOCfp9W",
    "outputId": "61763a4e-86c1-429e-820e-e77cc1a7592a"
   },
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    print((name, param.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhhtZuYDBCP6"
   },
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT ONNX FOR VISUALIZATION IN NETRON APP\n",
    "\n",
    "visualizationPath = '/content/drive/MyDrive/NeuroResearch/Data/eegCompress/models/model.onnx'\n",
    "dataset = datasetMake(torch.tensor(data[:,100:200]), model.numSampleInput, model.typeCode)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None)\n",
    "batch, label = next(loader.__iter__())\n",
    "yhat = model(batch)\n",
    "\n",
    "torch.onnx.export(model, batch, f=visualizationPath)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myPytorch",
   "language": "python",
   "name": "mypytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

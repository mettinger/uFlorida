{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5d3475a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.signal import spectrogram, stft, istft, check_NOLA\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision.transforms import ToTensor\n",
    "import os\n",
    "import neptune\n",
    "from neptune.utils import stringify_unsupported\n",
    "\n",
    "import scalpDeepModels as sdm\n",
    "\n",
    "import importlib\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdef38d8",
   "metadata": {},
   "source": [
    "# PARAMETERS - GENERAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1f63db3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "stftSavePath = '/blue/gkalamangalam/jmark.ettinger/predictScalp/freqRTheta.npz'\n",
    "timeDomainSavePath = '/blue/gkalamangalam/jmark.ettinger/predictScalp/timeDomain.npz'\n",
    "timeFreqSavePath = '/blue/gkalamangalam/jmark.ettinger/predictScalp/timeFreqRTheta.npz'\n",
    "\n",
    "modelPath = '/blue/gkalamangalam/jmark.ettinger/predictScalp/pytorchModels/model.pth'\n",
    "\n",
    "neptuneProject = 'jettinger35/predictScalp'\n",
    "api_token = os.environ.get('NEPTUNE_API_TOKEN')\n",
    "\n",
    "subsampleFreq = 128   # FINAL FREQUENCY IN HERTZ AFTER SUBSAMPLING\n",
    "secondsInWindow = 1\n",
    "nperseg = subsampleFreq * secondsInWindow\n",
    "noverlap = nperseg - 1\n",
    "window = ('tukey', .25)\n",
    "\n",
    "# Get cpu or gpu device for training.\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f7fce9",
   "metadata": {},
   "source": [
    "# PARAMETERS - TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "224077ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5000\n",
    "batch_size = 1024\n",
    "learningRate = 1e-3\n",
    "#loss_fn = nn.MSELoss()\n",
    "loss_fn = nn.L1Loss()\n",
    "optChoice = 'adam'\n",
    "\n",
    "patience = 5000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6db8c0",
   "metadata": {},
   "source": [
    "# UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe7e2826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERT STFT FROM R,THETA TO COMPLEX\n",
    "# dim(z) = (# timesteps, # freq bins x 2 (2 reals = 1 complex))\n",
    "\n",
    "def rThetaToComplex(z):\n",
    "    rows, cols = z.shape\n",
    "    shortTermFourier = np.zeros((rows, cols // 2), dtype=np.csingle)\n",
    "    for i in range(rows):\n",
    "        for k in range(cols // 2):\n",
    "            r = z[i,k]\n",
    "            theta = z[i, (k + cols // 2)]\n",
    "            shortTermFourier[i,k] =  r * np.exp(complex(0, theta))\n",
    "    return shortTermFourier.transpose() # dim = (# freq bins, # timepoints)\n",
    "\n",
    "# CONVERT REAL STFT TO COMPLEX STFT, INVERT TO GET THE ISTFT (I.E. TIME SERIES), THEN PLOT\n",
    "\n",
    "def realSTFTtoTimeSeries(realSTFT):\n",
    "    shortTermFourierComplex = rThetaToComplex(realSTFT)\n",
    "    times, inverseShortFourier = istft(shortTermFourierComplex, \n",
    "                                       fs=subsampleFreq, \n",
    "                                       window=window, \n",
    "                                       nperseg=nperseg, \n",
    "                                       noverlap=noverlap)\n",
    "    return times, inverseShortFourier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbbd749",
   "metadata": {},
   "source": [
    "# LOAD NUMPY DATA ARRAYS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ba4b2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSwitch = 'time'\n",
    "\n",
    "if dataSwitch == 'freq':\n",
    "    # STFT DATA\n",
    "\n",
    "    npzfile = np.load(stftSavePath)\n",
    "    x_trainRTheta = npzfile['x_trainRTheta']\n",
    "    x_validRTheta = npzfile['x_validRTheta'] \n",
    "    y_trainRTheta = npzfile['y_trainRTheta'] \n",
    "    y_validRTheta = npzfile['y_validRTheta']\n",
    "\n",
    "    trainXTensor = torch.Tensor(x_trainRTheta)\n",
    "    trainYTensor = torch.Tensor(y_trainRTheta)\n",
    "    validXTensor = torch.Tensor(x_validRTheta)\n",
    "    validYTensor = torch.Tensor(y_validRTheta)\n",
    "\n",
    "elif dataSwitch == 'time':\n",
    "    # TIME DOMAIN DATA\n",
    "\n",
    "    npzfile = np.load(timeDomainSavePath)\n",
    "    xTrainTimeDomain = npzfile['xTrainTimeDomain']\n",
    "    xValidTimeDomain = npzfile['xValidTimeDomain'] \n",
    "    yTrainTimeDomain = npzfile['yTrainTimeDomain'] \n",
    "    yValidTimeDomain = npzfile['yValidTimeDomain']\n",
    "\n",
    "    trainXTensor = torch.Tensor(xTrainTimeDomain)\n",
    "    trainYTensor = torch.Tensor(yTrainTimeDomain)\n",
    "    validXTensor = torch.Tensor(xValidTimeDomain)\n",
    "    validYTensor = torch.Tensor(yValidTimeDomain)\n",
    "    \n",
    "elif dataSwitch == 'timeFreq':\n",
    "    \n",
    "    npzfile = np.load(timeFreqSavePath)\n",
    "    xTrain = npzfile['x_trainTimeFreq']\n",
    "    xValid = npzfile['x_validTimeFreq'] \n",
    "    yTrain = npzfile['y_trainTimeFreq'] \n",
    "    yValid = npzfile['y_validTimeFreq']\n",
    "\n",
    "    trainXTensor = torch.Tensor(xTrain)\n",
    "    trainYTensor = torch.Tensor(yTrain)\n",
    "    validXTensor = torch.Tensor(xValid)\n",
    "    validYTensor = torch.Tensor(yValid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cedd53c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: \n",
      "Shape of X [N, C, H, W]: torch.Size([1024, 5655])\n",
      "Shape of y: torch.Size([1024, 1]) torch.float32\n",
      "\n",
      "test: \n",
      "Shape of X [N, C, H, W]: torch.Size([1024, 5655])\n",
      "Shape of y: torch.Size([1024, 1]) torch.float32\n"
     ]
    }
   ],
   "source": [
    "# CREATE PYTORCH DATALOADERS\n",
    "\n",
    "trainDataset = TensorDataset(trainXTensor,trainYTensor)\n",
    "trainDataLoader = DataLoader(trainDataset,batch_size=batch_size, shuffle=True)\n",
    "\n",
    "validDataset = TensorDataset(validXTensor,validYTensor)\n",
    "validDataLoader = DataLoader(validDataset,batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "print(\"train: \")\n",
    "for X, y in trainDataLoader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "    \n",
    "print(\"\\ntest: \")\n",
    "for X, y in validDataLoader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5243c05c",
   "metadata": {},
   "source": [
    "# DEFINE OR LOAD THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe9d0611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://new-ui.neptune.ai/jettinger35/predictScalp/e/PRED-43\n",
      "model download success...\n",
      "Shutting down background jobs, please wait a moment...\n",
      "Done!\n",
      "Waiting for the remaining 6 operations to synchronize with Neptune. Do not kill this process.\n",
      "All 6 operations synced, thanks for waiting!\n",
      "Explore the metadata in the Neptune app:\n",
      "https://new-ui.neptune.ai/jettinger35/predictScalp/e/PRED-43/metadata\n"
     ]
    }
   ],
   "source": [
    "run = neptune.init_run(\n",
    "    project=neptuneProject,\n",
    "    api_token=api_token,  \n",
    "    capture_hardware_metrics=True,\n",
    "    capture_stderr=True,\n",
    "    capture_stdout=True,\n",
    "    with_id=\"PRED-43\"\n",
    ")\n",
    "\n",
    "try:\n",
    "    destinationPathModel = modelPath\n",
    "    run[\"model_best\"].download(destinationPathModel)\n",
    "    print(\"model download success...\")\n",
    "    run.stop()\n",
    "except Exception as error:\n",
    "    print(\"model download failure...\")\n",
    "    print(error)\n",
    "    run.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fa50b8be",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2801460522.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[23], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    (model): Sequential(\u001b[0m\n\u001b[0m           ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(\n",
    "  (model): Sequential(\n",
    "    (bn0): BatchNorm1d(5655, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (l0): Linear(in_features=5655, out_features=512, bias=True)\n",
    "    (r0): ReLU()\n",
    "    (d0): Dropout(p=0.5, inplace=False)\n",
    "    (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (l1): Linear(in_features=512, out_features=512, bias=True)\n",
    "    (r1): ReLU()\n",
    "    (d1): Dropout(p=0.5, inplace=False)\n",
    "    (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (l2): Linear(in_features=512, out_features=512, bias=True)\n",
    "    (r2): ReLU()\n",
    "    (d2): Dropout(p=0.5, inplace=False)\n",
    "    (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (l3): Linear(in_features=512, out_features=512, bias=True)\n",
    "    (r3): ReLU()\n",
    "    (d3): Dropout(p=0.5, inplace=False)\n",
    "    (bn4): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (l4): Linear(in_features=512, out_features=512, bias=True)\n",
    "    (r4): ReLU()\n",
    "    (d4): Dropout(p=0.5, inplace=False)\n",
    "    (bn5): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "    (l5): Linear(in_features=512, out_features=1, bias=True)\n",
    "  )\n",
    ")\n",
    "\n",
    "\n",
    "Sequential(\n",
    "  (in): Linear(in_features=5655, out_features=512, bias=True)\n",
    "  (bn0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (l0): Linear(in_features=512, out_features=512, bias=True)\n",
    "  (r0): ReLU()\n",
    "  (d0): Dropout(p=0.5, inplace=False)\n",
    "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (l1): Linear(in_features=512, out_features=512, bias=True)\n",
    "  (r1): ReLU()\n",
    "  (d1): Dropout(p=0.5, inplace=False)\n",
    "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (l2): Linear(in_features=512, out_features=512, bias=True)\n",
    "  (r2): ReLU()\n",
    "  (d2): Dropout(p=0.5, inplace=False)\n",
    "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
    "  (l3): Linear(in_features=512, out_features=512, bias=True)\n",
    "  (out): Linear(in_features=512, out_features=1, bias=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eeaad9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  3951105\n",
      "Sequential(\n",
      "  (in): Linear(in_features=5655, out_features=512, bias=True)\n",
      "  (bn0): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (l0): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (r0): ReLU()\n",
      "  (d0): Dropout(p=0.5, inplace=False)\n",
      "  (bn1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (l1): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (r1): ReLU()\n",
      "  (d1): Dropout(p=0.5, inplace=False)\n",
      "  (bn2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (l2): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (r2): ReLU()\n",
      "  (d2): Dropout(p=0.5, inplace=False)\n",
      "  (bn3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (l3): Linear(in_features=512, out_features=512, bias=True)\n",
      "  (out): Linear(in_features=512, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# DEFINE MODEL\n",
    "\n",
    "modelLoadFlag = False\n",
    "\n",
    "if modelLoadFlag == True:\n",
    "    model = torch.load(modelPath)\n",
    "    bestTestLoss = sdm.test(validDataLoader, model, loss_fn)\n",
    "else:\n",
    "    importlib.reload(sdm) # reload in case we've made any architecture changes\n",
    "    \n",
    "    # DEFINE ARCHITECTURE HERE\n",
    "    inputSize = trainXTensor.shape[1]\n",
    "    hiddenLayerSizes = [512,512,512,512,512]\n",
    "    layerDict = sdm.listToOrderedDict_1(inputSize, hiddenLayerSizes)\n",
    "    #layerDict = sdm.residualAddDict(inputSize, 512, 5)\n",
    "    #layerDict = sdm.residualConcatDict(inputSize, hiddenLayerSizes)\n",
    "    model = nn.Sequential(layerDict)\n",
    "    bestTestLoss = float('inf')\n",
    "    \n",
    "print(\"Number of parameters: \", sdm.count_parameters(model))\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df68e5ed",
   "metadata": {},
   "source": [
    "# TRAIN (LOG DATA TO NEPTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64c6012d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://new-ui.neptune.ai/jettinger35/predictScalp/e/PRED-54\n",
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 0.791030  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 10.295714 \n",
      "\n",
      "\n",
      "Saved a new best model!\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 10.402260  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 2.547617 \n",
      "\n",
      "\n",
      "Saved a new best model!\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 2.569907  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 4.647707 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 3.956735  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 3.747100 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 3.369908  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 1.226932 \n",
      "\n",
      "\n",
      "Saved a new best model!\n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.160034  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 2.090100 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.977009  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 2.399533 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.897463  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 1.077463 \n",
      "\n",
      "\n",
      "Saved a new best model!\n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "File /blue/gkalamangalam/jmark.ettinger/predictScalp/pytorchModels/model.pth changed during upload, restarting upload.\n",
      "loss: 0.689368  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.747927 \n",
      "\n",
      "File /blue/gkalamangalam/jmark.ettinger/predictScalp/pytorchModels/model.pth changed during upload, restarting upload.\n",
      "\n",
      "Saved a new best model!\n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 1.399231  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.769069 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 1.180093  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.960280 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.962261  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 1.318637 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 1.194088  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.967121 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.922196  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.808930 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.881695  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.734453 \n",
      "\n",
      "\n",
      "Saved a new best model!\n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.929159  [ 1024/176128]\n",
      "File /blue/gkalamangalam/jmark.ettinger/predictScalp/pytorchModels/model.pth changed during upload, restarting upload.\n",
      "Test Error: \n",
      " Avg loss: 0.932737 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.996133  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.909717 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.926990  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.821423 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.802635  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.925885 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.971007  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.877722 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.966465  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.672323 \n",
      "\n",
      "\n",
      "Saved a new best model!\n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.737542  [ 1024/176128]\n",
      "File /blue/gkalamangalam/jmark.ettinger/predictScalp/pytorchModels/model.pth changed during upload, restarting upload.\n",
      "Test Error: \n",
      " Avg loss: 0.735680 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.754958  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.857815 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.879430  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.771387 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.792192  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.521963 \n",
      "\n",
      "File /blue/gkalamangalam/jmark.ettinger/predictScalp/pytorchModels/model.pth changed during upload, restarting upload.\n",
      "\n",
      "Saved a new best model!\n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.525166  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.747528 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.727396  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.725971 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.759801  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.478256 \n",
      "\n",
      "\n",
      "Saved a new best model!\n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.469710  [ 1024/176128]\n",
      "File /blue/gkalamangalam/jmark.ettinger/predictScalp/pytorchModels/model.pth changed during upload, restarting upload.\n",
      "Test Error: \n",
      " Avg loss: 0.628030 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.607629  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.633986 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.655834  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.505317 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.528821  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.559502 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.528592  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.645469 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.678403  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.517143 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.502093  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.484095 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.485706  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.575617 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.623285  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.494757 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.482987  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.489445 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.490656  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.530884 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.520823  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.530874 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.508346  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.451812 \n",
      "\n",
      "\n",
      "Saved a new best model!\n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.474436  [ 1024/176128]\n",
      "File /blue/gkalamangalam/jmark.ettinger/predictScalp/pytorchModels/model.pth changed during upload, restarting upload.\n",
      "Test Error: \n",
      " Avg loss: 0.474212 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.485622  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.478680 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.470045  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.454673 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.453603  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.487574 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.446816  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.516358 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.520619  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.466184 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.430591  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.448775 \n",
      "\n",
      "\n",
      "Saved a new best model!\n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.423359  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.458979 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.440089  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.447288 \n",
      "\n",
      "\n",
      "Saved a new best model!\n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.435978  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.442789 \n",
      "\n",
      "\n",
      "Saved a new best model!\n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.415074  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.467749 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.427520  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.456402 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.436720  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.436069 \n",
      "\n",
      "File /blue/gkalamangalam/jmark.ettinger/predictScalp/pytorchModels/model.pth changed during upload, restarting upload.\n",
      "\n",
      "Saved a new best model!\n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.427304  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.425292 \n",
      "\n",
      "\n",
      "Saved a new best model!\n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.412409  [ 1024/176128]\n",
      "File /blue/gkalamangalam/jmark.ettinger/predictScalp/pytorchModels/model.pth changed during upload, restarting upload.\n",
      "Test Error: \n",
      " Avg loss: 0.421365 \n",
      "\n",
      "\n",
      "Saved a new best model!\n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.396312  [ 1024/176128]\n",
      "File /blue/gkalamangalam/jmark.ettinger/predictScalp/pytorchModels/model.pth changed during upload, restarting upload.\n",
      "Test Error: \n",
      " Avg loss: 0.427041 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.378547  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.434713 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.409065  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.428099 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.400951  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.422761 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.403777  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.426763 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.416494  [ 1024/176128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 0.440782 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.419894  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.432032 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.416068  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.436964 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.396233  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.435962 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.411000  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.440059 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.401515  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.417616 \n",
      "\n",
      "\n",
      "Saved a new best model!\n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.399112  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.417394 \n",
      "\n",
      "\n",
      "Saved a new best model!\n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "File /blue/gkalamangalam/jmark.ettinger/predictScalp/pytorchModels/model.pth changed during upload, restarting upload.\n",
      "loss: 0.410632  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.420506 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.406288  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.422422 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.390908  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.431020 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.394805  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.427763 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.371678  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.418346 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.378600  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.426459 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.397967  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.432628 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.397096  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.421453 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.371936  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.422965 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.403812  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.423196 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.396839  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.440017 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.422279  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.422675 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.392506  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.417504 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.396453  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.429971 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.416901  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.425526 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.397195  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.418237 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.381491  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.421221 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.408118  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.446264 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.410050  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.433758 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.378017  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.443258 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.405367  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.421986 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.408501  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.442273 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.401285  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.437450 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.423581  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.421210 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.377328  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.450934 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.395974  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.441548 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.403894  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.432724 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.406921  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.423869 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.365243  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.431563 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.382871  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.433028 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.370678  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.442571 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.391602  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.433150 \n",
      "\n",
      "Epoch 101\n",
      "-------------------------------\n",
      "loss: 0.384808  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.460168 \n",
      "\n",
      "Epoch 102\n",
      "-------------------------------\n",
      "loss: 0.432882  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.431081 \n",
      "\n",
      "Epoch 103\n",
      "-------------------------------\n",
      "loss: 0.412212  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.474648 \n",
      "\n",
      "Epoch 104\n",
      "-------------------------------\n",
      "loss: 0.467176  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.438300 \n",
      "\n",
      "Epoch 105\n",
      "-------------------------------\n",
      "loss: 0.408230  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.526158 \n",
      "\n",
      "Epoch 106\n",
      "-------------------------------\n",
      "loss: 0.491232  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.464740 \n",
      "\n",
      "Epoch 107\n",
      "-------------------------------\n",
      "loss: 0.428658  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.482308 \n",
      "\n",
      "Epoch 108\n",
      "-------------------------------\n",
      "loss: 0.457681  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.476774 \n",
      "\n",
      "Epoch 109\n",
      "-------------------------------\n",
      "loss: 0.441460  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.448689 \n",
      "\n",
      "Epoch 110\n",
      "-------------------------------\n",
      "loss: 0.391587  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.463016 \n",
      "\n",
      "Epoch 111\n",
      "-------------------------------\n",
      "loss: 0.446741  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.430140 \n",
      "\n",
      "Epoch 112\n",
      "-------------------------------\n",
      "loss: 0.384166  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.442289 \n",
      "\n",
      "Epoch 113\n",
      "-------------------------------\n",
      "loss: 0.394468  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.453651 \n",
      "\n",
      "Epoch 114\n",
      "-------------------------------\n",
      "loss: 0.413600  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.480984 \n",
      "\n",
      "Epoch 115\n",
      "-------------------------------\n",
      "loss: 0.440870  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.438914 \n",
      "\n",
      "Epoch 116\n",
      "-------------------------------\n",
      "loss: 0.429594  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.447868 \n",
      "\n",
      "Epoch 117\n",
      "-------------------------------\n",
      "loss: 0.415197  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.438497 \n",
      "\n",
      "Epoch 118\n",
      "-------------------------------\n",
      "loss: 0.412940  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.438241 \n",
      "\n",
      "Epoch 119\n",
      "-------------------------------\n",
      "loss: 0.397927  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.424241 \n",
      "\n",
      "Epoch 120\n",
      "-------------------------------\n",
      "loss: 0.382870  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.429000 \n",
      "\n",
      "Epoch 121\n",
      "-------------------------------\n",
      "loss: 0.391872  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.443564 \n",
      "\n",
      "Epoch 122\n",
      "-------------------------------\n",
      "loss: 0.388864  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.445333 \n",
      "\n",
      "Epoch 123\n",
      "-------------------------------\n",
      "loss: 0.408768  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.421061 \n",
      "\n",
      "Epoch 124\n",
      "-------------------------------\n",
      "loss: 0.393777  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.426305 \n",
      "\n",
      "Epoch 125\n",
      "-------------------------------\n",
      "loss: 0.406542  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.433410 \n",
      "\n",
      "Epoch 126\n",
      "-------------------------------\n",
      "loss: 0.391988  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.435635 \n",
      "\n",
      "Epoch 127\n",
      "-------------------------------\n",
      "loss: 0.397251  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.410395 \n",
      "\n",
      "\n",
      "Saved a new best model!\n",
      "\n",
      "Epoch 128\n",
      "-------------------------------\n",
      "loss: 0.349810  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.428173 \n",
      "\n",
      "Epoch 129\n",
      "-------------------------------\n",
      "loss: 0.382292  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.421519 \n",
      "\n",
      "Epoch 130\n",
      "-------------------------------\n",
      "loss: 0.343322  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.436462 \n",
      "\n",
      "Epoch 131\n",
      "-------------------------------\n",
      "loss: 0.369808  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.430940 \n",
      "\n",
      "Epoch 132\n",
      "-------------------------------\n",
      "loss: 0.382254  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.419430 \n",
      "\n",
      "Epoch 133\n",
      "-------------------------------\n",
      "loss: 0.368452  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.411587 \n",
      "\n",
      "Epoch 134\n",
      "-------------------------------\n",
      "loss: 0.382866  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.426905 \n",
      "\n",
      "Epoch 135\n",
      "-------------------------------\n",
      "loss: 0.367006  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.418130 \n",
      "\n",
      "Epoch 136\n",
      "-------------------------------\n",
      "loss: 0.335720  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.424542 \n",
      "\n",
      "Epoch 137\n",
      "-------------------------------\n",
      "loss: 0.369914  [ 1024/176128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 0.412946 \n",
      "\n",
      "Epoch 138\n",
      "-------------------------------\n",
      "loss: 0.380495  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.434890 \n",
      "\n",
      "Epoch 139\n",
      "-------------------------------\n",
      "loss: 0.414510  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.414605 \n",
      "\n",
      "Epoch 140\n",
      "-------------------------------\n",
      "loss: 0.343583  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.415948 \n",
      "\n",
      "Epoch 141\n",
      "-------------------------------\n",
      "loss: 0.359177  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.416079 \n",
      "\n",
      "Epoch 142\n",
      "-------------------------------\n",
      "loss: 0.347022  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.437940 \n",
      "\n",
      "Epoch 143\n",
      "-------------------------------\n",
      "loss: 0.369510  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.418840 \n",
      "\n",
      "Epoch 144\n",
      "-------------------------------\n",
      "loss: 0.357942  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.421062 \n",
      "\n",
      "Epoch 145\n",
      "-------------------------------\n",
      "loss: 0.379026  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.428634 \n",
      "\n",
      "Epoch 146\n",
      "-------------------------------\n",
      "loss: 0.361658  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.432387 \n",
      "\n",
      "Epoch 147\n",
      "-------------------------------\n",
      "loss: 0.373320  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.415405 \n",
      "\n",
      "Epoch 148\n",
      "-------------------------------\n",
      "loss: 0.387818  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.412346 \n",
      "\n",
      "Epoch 149\n",
      "-------------------------------\n",
      "loss: 0.363476  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.441189 \n",
      "\n",
      "Epoch 150\n",
      "-------------------------------\n",
      "loss: 0.387438  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.415482 \n",
      "\n",
      "Epoch 151\n",
      "-------------------------------\n",
      "loss: 0.349893  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.433796 \n",
      "\n",
      "Epoch 152\n",
      "-------------------------------\n",
      "loss: 0.396452  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.419691 \n",
      "\n",
      "Epoch 153\n",
      "-------------------------------\n",
      "loss: 0.346001  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.430152 \n",
      "\n",
      "Epoch 154\n",
      "-------------------------------\n",
      "loss: 0.377274  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.432880 \n",
      "\n",
      "Epoch 155\n",
      "-------------------------------\n",
      "loss: 0.385162  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.412104 \n",
      "\n",
      "Epoch 156\n",
      "-------------------------------\n",
      "loss: 0.338007  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.445956 \n",
      "\n",
      "Epoch 157\n",
      "-------------------------------\n",
      "loss: 0.363685  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.420233 \n",
      "\n",
      "Epoch 158\n",
      "-------------------------------\n",
      "loss: 0.358768  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.471506 \n",
      "\n",
      "Epoch 159\n",
      "-------------------------------\n",
      "loss: 0.425625  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.416301 \n",
      "\n",
      "Epoch 160\n",
      "-------------------------------\n",
      "loss: 0.379683  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.514165 \n",
      "\n",
      "Epoch 161\n",
      "-------------------------------\n",
      "loss: 0.474231  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.453088 \n",
      "\n",
      "Epoch 162\n",
      "-------------------------------\n",
      "loss: 0.400539  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.482389 \n",
      "\n",
      "Epoch 163\n",
      "-------------------------------\n",
      "loss: 0.456285  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.463620 \n",
      "\n",
      "Epoch 164\n",
      "-------------------------------\n",
      "loss: 0.413718  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.455660 \n",
      "\n",
      "Epoch 165\n",
      "-------------------------------\n",
      "loss: 0.381464  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.487421 \n",
      "\n",
      "Epoch 166\n",
      "-------------------------------\n",
      "loss: 0.452695  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.419703 \n",
      "\n",
      "Epoch 167\n",
      "-------------------------------\n",
      "loss: 0.361443  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.434576 \n",
      "\n",
      "Epoch 168\n",
      "-------------------------------\n",
      "loss: 0.376392  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.411168 \n",
      "\n",
      "Epoch 169\n",
      "-------------------------------\n",
      "loss: 0.339396  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.434478 \n",
      "\n",
      "Epoch 170\n",
      "-------------------------------\n",
      "loss: 0.369072  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.404136 \n",
      "\n",
      "\n",
      "Saved a new best model!\n",
      "\n",
      "Epoch 171\n",
      "-------------------------------\n",
      "loss: 0.351515  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.419744 \n",
      "\n",
      "Epoch 172\n",
      "-------------------------------\n",
      "loss: 0.365544  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.410222 \n",
      "\n",
      "Epoch 173\n",
      "-------------------------------\n",
      "loss: 0.347779  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.420057 \n",
      "\n",
      "Epoch 174\n",
      "-------------------------------\n",
      "loss: 0.352096  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.416789 \n",
      "\n",
      "Epoch 175\n",
      "-------------------------------\n",
      "loss: 0.363060  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.428050 \n",
      "\n",
      "Epoch 176\n",
      "-------------------------------\n",
      "loss: 0.377281  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.409440 \n",
      "\n",
      "Epoch 177\n",
      "-------------------------------\n",
      "loss: 0.351085  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.421372 \n",
      "\n",
      "Epoch 178\n",
      "-------------------------------\n",
      "loss: 0.342718  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.413374 \n",
      "\n",
      "Epoch 179\n",
      "-------------------------------\n",
      "loss: 0.357089  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.410808 \n",
      "\n",
      "Epoch 180\n",
      "-------------------------------\n",
      "loss: 0.358609  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.423679 \n",
      "\n",
      "Epoch 181\n",
      "-------------------------------\n",
      "loss: 0.363144  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.416693 \n",
      "\n",
      "Epoch 182\n",
      "-------------------------------\n",
      "loss: 0.353342  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.434029 \n",
      "\n",
      "Epoch 183\n",
      "-------------------------------\n",
      "loss: 0.361347  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.414761 \n",
      "\n",
      "Epoch 184\n",
      "-------------------------------\n",
      "loss: 0.355898  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.425125 \n",
      "\n",
      "Epoch 185\n",
      "-------------------------------\n",
      "loss: 0.366107  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.430097 \n",
      "\n",
      "Epoch 186\n",
      "-------------------------------\n",
      "loss: 0.371458  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.411084 \n",
      "\n",
      "Epoch 187\n",
      "-------------------------------\n",
      "loss: 0.350873  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.419087 \n",
      "\n",
      "Epoch 188\n",
      "-------------------------------\n",
      "loss: 0.346475  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.423669 \n",
      "\n",
      "Epoch 189\n",
      "-------------------------------\n",
      "loss: 0.349909  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.411189 \n",
      "\n",
      "Epoch 190\n",
      "-------------------------------\n",
      "loss: 0.361531  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.419193 \n",
      "\n",
      "Epoch 191\n",
      "-------------------------------\n",
      "loss: 0.370483  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.412213 \n",
      "\n",
      "Epoch 192\n",
      "-------------------------------\n",
      "loss: 0.340101  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.416460 \n",
      "\n",
      "Epoch 193\n",
      "-------------------------------\n",
      "loss: 0.371002  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.429185 \n",
      "\n",
      "Epoch 194\n",
      "-------------------------------\n",
      "loss: 0.361547  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.423378 \n",
      "\n",
      "Epoch 195\n",
      "-------------------------------\n",
      "loss: 0.350704  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.425640 \n",
      "\n",
      "Epoch 196\n",
      "-------------------------------\n",
      "loss: 0.370089  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.435553 \n",
      "\n",
      "Epoch 197\n",
      "-------------------------------\n",
      "loss: 0.374886  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.417516 \n",
      "\n",
      "Epoch 198\n",
      "-------------------------------\n",
      "loss: 0.347030  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.427148 \n",
      "\n",
      "Epoch 199\n",
      "-------------------------------\n",
      "loss: 0.372423  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.422809 \n",
      "\n",
      "Epoch 200\n",
      "-------------------------------\n",
      "loss: 0.350419  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.421414 \n",
      "\n",
      "Epoch 201\n",
      "-------------------------------\n",
      "loss: 0.360865  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.426562 \n",
      "\n",
      "Epoch 202\n",
      "-------------------------------\n",
      "loss: 0.343048  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.414841 \n",
      "\n",
      "Epoch 203\n",
      "-------------------------------\n",
      "loss: 0.321581  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.423424 \n",
      "\n",
      "Epoch 204\n",
      "-------------------------------\n",
      "loss: 0.357210  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.421612 \n",
      "\n",
      "Epoch 205\n",
      "-------------------------------\n",
      "loss: 0.338541  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.414215 \n",
      "\n",
      "Epoch 206\n",
      "-------------------------------\n",
      "loss: 0.340507  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.419264 \n",
      "\n",
      "Epoch 207\n",
      "-------------------------------\n",
      "loss: 0.342063  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.415352 \n",
      "\n",
      "Epoch 208\n",
      "-------------------------------\n",
      "loss: 0.330982  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.427287 \n",
      "\n",
      "Epoch 209\n",
      "-------------------------------\n",
      "loss: 0.343073  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.417994 \n",
      "\n",
      "Epoch 210\n",
      "-------------------------------\n",
      "loss: 0.340458  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.418545 \n",
      "\n",
      "Epoch 211\n",
      "-------------------------------\n",
      "loss: 0.338911  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.414644 \n",
      "\n",
      "Epoch 212\n",
      "-------------------------------\n",
      "loss: 0.326323  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.421011 \n",
      "\n",
      "Epoch 213\n",
      "-------------------------------\n",
      "loss: 0.337410  [ 1024/176128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 0.435036 \n",
      "\n",
      "Epoch 214\n",
      "-------------------------------\n",
      "loss: 0.352047  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.422822 \n",
      "\n",
      "Epoch 215\n",
      "-------------------------------\n",
      "loss: 0.353476  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.438494 \n",
      "\n",
      "Epoch 216\n",
      "-------------------------------\n",
      "loss: 0.350365  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.417827 \n",
      "\n",
      "Epoch 217\n",
      "-------------------------------\n",
      "loss: 0.325515  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.444287 \n",
      "\n",
      "Epoch 218\n",
      "-------------------------------\n",
      "loss: 0.394912  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.445041 \n",
      "\n",
      "Epoch 219\n",
      "-------------------------------\n",
      "loss: 0.373306  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.433391 \n",
      "\n",
      "Epoch 220\n",
      "-------------------------------\n",
      "loss: 0.352968  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.446151 \n",
      "\n",
      "Epoch 221\n",
      "-------------------------------\n",
      "loss: 0.390013  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.413044 \n",
      "\n",
      "Epoch 222\n",
      "-------------------------------\n",
      "loss: 0.342405  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.469555 \n",
      "\n",
      "Epoch 223\n",
      "-------------------------------\n",
      "loss: 0.414690  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.417863 \n",
      "\n",
      "Epoch 224\n",
      "-------------------------------\n",
      "loss: 0.335636  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.439736 \n",
      "\n",
      "Epoch 225\n",
      "-------------------------------\n",
      "loss: 0.375308  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.443257 \n",
      "\n",
      "Epoch 226\n",
      "-------------------------------\n",
      "loss: 0.370059  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.435285 \n",
      "\n",
      "Epoch 227\n",
      "-------------------------------\n",
      "loss: 0.367608  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.452708 \n",
      "\n",
      "Epoch 228\n",
      "-------------------------------\n",
      "loss: 0.381475  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.413393 \n",
      "\n",
      "Epoch 229\n",
      "-------------------------------\n",
      "loss: 0.345911  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.447130 \n",
      "\n",
      "Epoch 230\n",
      "-------------------------------\n",
      "loss: 0.367132  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.422959 \n",
      "\n",
      "Epoch 231\n",
      "-------------------------------\n",
      "loss: 0.344717  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.433772 \n",
      "\n",
      "Epoch 232\n",
      "-------------------------------\n",
      "loss: 0.369430  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.449295 \n",
      "\n",
      "Epoch 233\n",
      "-------------------------------\n",
      "loss: 0.395633  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.425536 \n",
      "\n",
      "Epoch 234\n",
      "-------------------------------\n",
      "loss: 0.346751  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.449940 \n",
      "\n",
      "Epoch 235\n",
      "-------------------------------\n",
      "loss: 0.377650  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.430303 \n",
      "\n",
      "Epoch 236\n",
      "-------------------------------\n",
      "loss: 0.344399  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.471609 \n",
      "\n",
      "Epoch 237\n",
      "-------------------------------\n",
      "loss: 0.415064  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.441785 \n",
      "\n",
      "Epoch 238\n",
      "-------------------------------\n",
      "loss: 0.373219  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.422455 \n",
      "\n",
      "Epoch 239\n",
      "-------------------------------\n",
      "loss: 0.349947  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.427541 \n",
      "\n",
      "Epoch 240\n",
      "-------------------------------\n",
      "loss: 0.358883  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.425789 \n",
      "\n",
      "Epoch 241\n",
      "-------------------------------\n",
      "loss: 0.342470  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.419360 \n",
      "\n",
      "Epoch 242\n",
      "-------------------------------\n",
      "loss: 0.344199  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.420431 \n",
      "\n",
      "Epoch 243\n",
      "-------------------------------\n",
      "loss: 0.336551  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.425255 \n",
      "\n",
      "Epoch 244\n",
      "-------------------------------\n",
      "loss: 0.350300  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.419559 \n",
      "\n",
      "Epoch 245\n",
      "-------------------------------\n",
      "loss: 0.325573  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.421651 \n",
      "\n",
      "Epoch 246\n",
      "-------------------------------\n",
      "loss: 0.350801  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.426771 \n",
      "\n",
      "Epoch 247\n",
      "-------------------------------\n",
      "loss: 0.329053  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.427235 \n",
      "\n",
      "Epoch 248\n",
      "-------------------------------\n",
      "loss: 0.336745  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.440660 \n",
      "\n",
      "Epoch 249\n",
      "-------------------------------\n",
      "loss: 0.373539  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.418769 \n",
      "\n",
      "Epoch 250\n",
      "-------------------------------\n",
      "loss: 0.331927  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.419368 \n",
      "\n",
      "Epoch 251\n",
      "-------------------------------\n",
      "loss: 0.322669  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.425959 \n",
      "\n",
      "Epoch 252\n",
      "-------------------------------\n",
      "loss: 0.340300  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.417573 \n",
      "\n",
      "Epoch 253\n",
      "-------------------------------\n",
      "loss: 0.325515  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.433721 \n",
      "\n",
      "Epoch 254\n",
      "-------------------------------\n",
      "loss: 0.321122  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.431144 \n",
      "\n",
      "Epoch 255\n",
      "-------------------------------\n",
      "loss: 0.354274  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.429566 \n",
      "\n",
      "Epoch 256\n",
      "-------------------------------\n",
      "loss: 0.331695  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.429784 \n",
      "\n",
      "Epoch 257\n",
      "-------------------------------\n",
      "loss: 0.353672  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.423104 \n",
      "\n",
      "Epoch 258\n",
      "-------------------------------\n",
      "loss: 0.330925  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.419364 \n",
      "\n",
      "Epoch 259\n",
      "-------------------------------\n",
      "loss: 0.321886  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.425775 \n",
      "\n",
      "Epoch 260\n",
      "-------------------------------\n",
      "loss: 0.320559  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.426571 \n",
      "\n",
      "Epoch 261\n",
      "-------------------------------\n",
      "loss: 0.357152  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.420156 \n",
      "\n",
      "Epoch 262\n",
      "-------------------------------\n",
      "loss: 0.320934  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.429886 \n",
      "\n",
      "Epoch 263\n",
      "-------------------------------\n",
      "loss: 0.322649  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.440292 \n",
      "\n",
      "Epoch 264\n",
      "-------------------------------\n",
      "loss: 0.348716  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.427475 \n",
      "\n",
      "Epoch 265\n",
      "-------------------------------\n",
      "loss: 0.333042  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.426992 \n",
      "\n",
      "Epoch 266\n",
      "-------------------------------\n",
      "loss: 0.324862  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.419428 \n",
      "\n",
      "Epoch 267\n",
      "-------------------------------\n",
      "loss: 0.344969  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.418329 \n",
      "\n",
      "Epoch 268\n",
      "-------------------------------\n",
      "loss: 0.321832  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.433254 \n",
      "\n",
      "Epoch 269\n",
      "-------------------------------\n",
      "loss: 0.341316  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.421649 \n",
      "\n",
      "Epoch 270\n",
      "-------------------------------\n",
      "loss: 0.335171  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.413545 \n",
      "\n",
      "Epoch 271\n",
      "-------------------------------\n",
      "loss: 0.307642  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.436366 \n",
      "\n",
      "Epoch 272\n",
      "-------------------------------\n",
      "loss: 0.350613  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.419057 \n",
      "\n",
      "Epoch 273\n",
      "-------------------------------\n",
      "loss: 0.320835  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.437442 \n",
      "\n",
      "Epoch 274\n",
      "-------------------------------\n",
      "loss: 0.359982  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.438974 \n",
      "\n",
      "Epoch 275\n",
      "-------------------------------\n",
      "loss: 0.352699  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.431095 \n",
      "\n",
      "Epoch 276\n",
      "-------------------------------\n",
      "loss: 0.356248  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.452234 \n",
      "\n",
      "Epoch 277\n",
      "-------------------------------\n",
      "loss: 0.373586  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.430444 \n",
      "\n",
      "Epoch 278\n",
      "-------------------------------\n",
      "loss: 0.349648  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.434499 \n",
      "\n",
      "Epoch 279\n",
      "-------------------------------\n",
      "loss: 0.346883  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.425001 \n",
      "\n",
      "Epoch 280\n",
      "-------------------------------\n",
      "loss: 0.346067  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.440750 \n",
      "\n",
      "Epoch 281\n",
      "-------------------------------\n",
      "loss: 0.350233  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.425351 \n",
      "\n",
      "Epoch 282\n",
      "-------------------------------\n",
      "loss: 0.339233  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.437689 \n",
      "\n",
      "Epoch 283\n",
      "-------------------------------\n",
      "loss: 0.329103  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.425434 \n",
      "\n",
      "Epoch 284\n",
      "-------------------------------\n",
      "loss: 0.326675  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.433134 \n",
      "\n",
      "Epoch 285\n",
      "-------------------------------\n",
      "loss: 0.337259  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.417158 \n",
      "\n",
      "Epoch 286\n",
      "-------------------------------\n",
      "loss: 0.313918  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.416719 \n",
      "\n",
      "Epoch 287\n",
      "-------------------------------\n",
      "loss: 0.317781  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.419277 \n",
      "\n",
      "Epoch 288\n",
      "-------------------------------\n",
      "loss: 0.329834  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.423868 \n",
      "\n",
      "Epoch 289\n",
      "-------------------------------\n",
      "loss: 0.319069  [ 1024/176128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 0.414295 \n",
      "\n",
      "Epoch 290\n",
      "-------------------------------\n",
      "loss: 0.304983  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.419369 \n",
      "\n",
      "Epoch 291\n",
      "-------------------------------\n",
      "loss: 0.315136  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.408983 \n",
      "\n",
      "Epoch 292\n",
      "-------------------------------\n",
      "loss: 0.303111  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.408930 \n",
      "\n",
      "Epoch 293\n",
      "-------------------------------\n",
      "loss: 0.299806  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.414634 \n",
      "\n",
      "Epoch 294\n",
      "-------------------------------\n",
      "loss: 0.312400  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.417148 \n",
      "\n",
      "Epoch 295\n",
      "-------------------------------\n",
      "loss: 0.313937  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.420056 \n",
      "\n",
      "Epoch 296\n",
      "-------------------------------\n",
      "loss: 0.305063  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.422166 \n",
      "\n",
      "Epoch 297\n",
      "-------------------------------\n",
      "loss: 0.318786  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.430743 \n",
      "\n",
      "Epoch 298\n",
      "-------------------------------\n",
      "loss: 0.315312  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.424018 \n",
      "\n",
      "Epoch 299\n",
      "-------------------------------\n",
      "loss: 0.314153  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.434490 \n",
      "\n",
      "Epoch 300\n",
      "-------------------------------\n",
      "loss: 0.348284  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.428955 \n",
      "\n",
      "Epoch 301\n",
      "-------------------------------\n",
      "loss: 0.338082  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.451154 \n",
      "\n",
      "Epoch 302\n",
      "-------------------------------\n",
      "loss: 0.362931  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.420056 \n",
      "\n",
      "Epoch 303\n",
      "-------------------------------\n",
      "loss: 0.319747  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.454143 \n",
      "\n",
      "Epoch 304\n",
      "-------------------------------\n",
      "loss: 0.358953  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.431114 \n",
      "\n",
      "Epoch 305\n",
      "-------------------------------\n",
      "loss: 0.317035  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.422956 \n",
      "\n",
      "Epoch 306\n",
      "-------------------------------\n",
      "loss: 0.295369  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.429733 \n",
      "\n",
      "Epoch 307\n",
      "-------------------------------\n",
      "loss: 0.325701  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.432586 \n",
      "\n",
      "Epoch 308\n",
      "-------------------------------\n",
      "loss: 0.326274  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.432367 \n",
      "\n",
      "Epoch 309\n",
      "-------------------------------\n",
      "loss: 0.314958  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.444511 \n",
      "\n",
      "Epoch 310\n",
      "-------------------------------\n",
      "loss: 0.350283  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.426208 \n",
      "\n",
      "Epoch 311\n",
      "-------------------------------\n",
      "loss: 0.337841  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.425438 \n",
      "\n",
      "Epoch 312\n",
      "-------------------------------\n",
      "loss: 0.317817  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.429270 \n",
      "\n",
      "Epoch 313\n",
      "-------------------------------\n",
      "loss: 0.316565  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.431761 \n",
      "\n",
      "Epoch 314\n",
      "-------------------------------\n",
      "loss: 0.336018  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.417042 \n",
      "\n",
      "Epoch 315\n",
      "-------------------------------\n",
      "loss: 0.305306  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.424912 \n",
      "\n",
      "Epoch 316\n",
      "-------------------------------\n",
      "loss: 0.328623  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.433318 \n",
      "\n",
      "Epoch 317\n",
      "-------------------------------\n",
      "loss: 0.318929  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.424201 \n",
      "\n",
      "Epoch 318\n",
      "-------------------------------\n",
      "loss: 0.316838  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.424679 \n",
      "\n",
      "Epoch 319\n",
      "-------------------------------\n",
      "loss: 0.322605  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.418124 \n",
      "\n",
      "Epoch 320\n",
      "-------------------------------\n",
      "loss: 0.299741  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.430237 \n",
      "\n",
      "Epoch 321\n",
      "-------------------------------\n",
      "loss: 0.317052  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.419739 \n",
      "\n",
      "Epoch 322\n",
      "-------------------------------\n",
      "loss: 0.301373  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.429801 \n",
      "\n",
      "Epoch 323\n",
      "-------------------------------\n",
      "loss: 0.326662  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.416241 \n",
      "\n",
      "Epoch 324\n",
      "-------------------------------\n",
      "loss: 0.300639  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.444124 \n",
      "\n",
      "Epoch 325\n",
      "-------------------------------\n",
      "loss: 0.352976  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.439681 \n",
      "\n",
      "Epoch 326\n",
      "-------------------------------\n",
      "loss: 0.335071  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.428521 \n",
      "\n",
      "Epoch 327\n",
      "-------------------------------\n",
      "loss: 0.311173  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.414133 \n",
      "\n",
      "Epoch 328\n",
      "-------------------------------\n",
      "loss: 0.288145  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.437957 \n",
      "\n",
      "Epoch 329\n",
      "-------------------------------\n",
      "loss: 0.354871  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.430544 \n",
      "\n",
      "Epoch 330\n",
      "-------------------------------\n",
      "loss: 0.313303  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.440460 \n",
      "\n",
      "Epoch 331\n",
      "-------------------------------\n",
      "loss: 0.346709  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.426612 \n",
      "\n",
      "Epoch 332\n",
      "-------------------------------\n",
      "loss: 0.334477  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.439028 \n",
      "\n",
      "Epoch 333\n",
      "-------------------------------\n",
      "loss: 0.342340  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.427519 \n",
      "\n",
      "Epoch 334\n",
      "-------------------------------\n",
      "loss: 0.296139  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.438935 \n",
      "\n",
      "Epoch 335\n",
      "-------------------------------\n",
      "loss: 0.307874  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.424227 \n",
      "\n",
      "Epoch 336\n",
      "-------------------------------\n",
      "loss: 0.316408  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.427630 \n",
      "\n",
      "Epoch 337\n",
      "-------------------------------\n",
      "loss: 0.320909  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.448831 \n",
      "\n",
      "Epoch 338\n",
      "-------------------------------\n",
      "loss: 0.342590  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.490377 \n",
      "\n",
      "Epoch 339\n",
      "-------------------------------\n",
      "loss: 0.398262  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.435648 \n",
      "\n",
      "Epoch 340\n",
      "-------------------------------\n",
      "loss: 0.345493  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.516884 \n",
      "\n",
      "Epoch 341\n",
      "-------------------------------\n",
      "loss: 0.422115  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.426835 \n",
      "\n",
      "Epoch 342\n",
      "-------------------------------\n",
      "loss: 0.333203  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.457642 \n",
      "\n",
      "Epoch 343\n",
      "-------------------------------\n",
      "loss: 0.381013  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.451445 \n",
      "\n",
      "Epoch 344\n",
      "-------------------------------\n",
      "loss: 0.328960  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.428478 \n",
      "\n",
      "Epoch 345\n",
      "-------------------------------\n",
      "loss: 0.310641  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.439431 \n",
      "\n",
      "Epoch 346\n",
      "-------------------------------\n",
      "loss: 0.353441  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.418659 \n",
      "\n",
      "Epoch 347\n",
      "-------------------------------\n",
      "loss: 0.287182  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.462293 \n",
      "\n",
      "Epoch 348\n",
      "-------------------------------\n",
      "loss: 0.370450  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.420368 \n",
      "\n",
      "Epoch 349\n",
      "-------------------------------\n",
      "loss: 0.320464  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.422942 \n",
      "\n",
      "Epoch 350\n",
      "-------------------------------\n",
      "loss: 0.325303  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.433268 \n",
      "\n",
      "Epoch 351\n",
      "-------------------------------\n",
      "loss: 0.331546  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.415119 \n",
      "\n",
      "Epoch 352\n",
      "-------------------------------\n",
      "loss: 0.305649  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.409697 \n",
      "\n",
      "Epoch 353\n",
      "-------------------------------\n",
      "loss: 0.291580  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.421025 \n",
      "\n",
      "Epoch 354\n",
      "-------------------------------\n",
      "loss: 0.321282  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.410692 \n",
      "\n",
      "Epoch 355\n",
      "-------------------------------\n",
      "loss: 0.309921  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.413924 \n",
      "\n",
      "Epoch 356\n",
      "-------------------------------\n",
      "loss: 0.304651  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.447372 \n",
      "\n",
      "Epoch 357\n",
      "-------------------------------\n",
      "loss: 0.348043  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.444989 \n",
      "\n",
      "Epoch 358\n",
      "-------------------------------\n",
      "loss: 0.341859  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.422835 \n",
      "\n",
      "Epoch 359\n",
      "-------------------------------\n",
      "loss: 0.313144  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.444608 \n",
      "\n",
      "Epoch 360\n",
      "-------------------------------\n",
      "loss: 0.351912  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.464239 \n",
      "\n",
      "Epoch 361\n",
      "-------------------------------\n",
      "loss: 0.375460  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.431118 \n",
      "\n",
      "Epoch 362\n",
      "-------------------------------\n",
      "loss: 0.345534  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.485666 \n",
      "\n",
      "Epoch 363\n",
      "-------------------------------\n",
      "loss: 0.383309  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.434595 \n",
      "\n",
      "Epoch 364\n",
      "-------------------------------\n",
      "loss: 0.334115  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.421767 \n",
      "\n",
      "Epoch 365\n",
      "-------------------------------\n",
      "loss: 0.324607  [ 1024/176128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 0.449450 \n",
      "\n",
      "Epoch 366\n",
      "-------------------------------\n",
      "loss: 0.339558  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.420901 \n",
      "\n",
      "Epoch 367\n",
      "-------------------------------\n",
      "loss: 0.301574  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.445018 \n",
      "\n",
      "Epoch 368\n",
      "-------------------------------\n",
      "loss: 0.333489  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.451808 \n",
      "\n",
      "Epoch 369\n",
      "-------------------------------\n",
      "loss: 0.355386  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.431539 \n",
      "\n",
      "Epoch 370\n",
      "-------------------------------\n",
      "loss: 0.328301  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.425313 \n",
      "\n",
      "Epoch 371\n",
      "-------------------------------\n",
      "loss: 0.292490  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.434550 \n",
      "\n",
      "Epoch 372\n",
      "-------------------------------\n",
      "loss: 0.309259  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.418084 \n",
      "\n",
      "Epoch 373\n",
      "-------------------------------\n",
      "loss: 0.292752  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.420031 \n",
      "\n",
      "Epoch 374\n",
      "-------------------------------\n",
      "loss: 0.304241  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.419344 \n",
      "\n",
      "Epoch 375\n",
      "-------------------------------\n",
      "loss: 0.300728  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.426611 \n",
      "\n",
      "Epoch 376\n",
      "-------------------------------\n",
      "loss: 0.309808  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.422917 \n",
      "\n",
      "Epoch 377\n",
      "-------------------------------\n",
      "loss: 0.292342  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.432034 \n",
      "\n",
      "Epoch 378\n",
      "-------------------------------\n",
      "loss: 0.334525  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.437206 \n",
      "\n",
      "Epoch 379\n",
      "-------------------------------\n",
      "loss: 0.359321  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.419132 \n",
      "\n",
      "Epoch 380\n",
      "-------------------------------\n",
      "loss: 0.313091  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.415383 \n",
      "\n",
      "Epoch 381\n",
      "-------------------------------\n",
      "loss: 0.279959  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.419669 \n",
      "\n",
      "Epoch 382\n",
      "-------------------------------\n",
      "loss: 0.297905  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.413284 \n",
      "\n",
      "Epoch 383\n",
      "-------------------------------\n",
      "loss: 0.289015  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.418454 \n",
      "\n",
      "Epoch 384\n",
      "-------------------------------\n",
      "loss: 0.312779  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.424391 \n",
      "\n",
      "Epoch 385\n",
      "-------------------------------\n",
      "loss: 0.309528  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.420390 \n",
      "\n",
      "Epoch 386\n",
      "-------------------------------\n",
      "loss: 0.277142  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.430907 \n",
      "\n",
      "Epoch 387\n",
      "-------------------------------\n",
      "loss: 0.297929  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.442809 \n",
      "\n",
      "Epoch 388\n",
      "-------------------------------\n",
      "loss: 0.340172  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.420952 \n",
      "\n",
      "Epoch 389\n",
      "-------------------------------\n",
      "loss: 0.298630  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.442608 \n",
      "\n",
      "Epoch 390\n",
      "-------------------------------\n",
      "loss: 0.324088  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.459068 \n",
      "\n",
      "Epoch 391\n",
      "-------------------------------\n",
      "loss: 0.358598  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.419832 \n",
      "\n",
      "Epoch 392\n",
      "-------------------------------\n",
      "loss: 0.285591  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.443979 \n",
      "\n",
      "Epoch 393\n",
      "-------------------------------\n",
      "loss: 0.327773  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.453012 \n",
      "\n",
      "Epoch 394\n",
      "-------------------------------\n",
      "loss: 0.350734  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.424792 \n",
      "\n",
      "Epoch 395\n",
      "-------------------------------\n",
      "loss: 0.322999  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.495014 \n",
      "\n",
      "Epoch 396\n",
      "-------------------------------\n",
      "loss: 0.399436  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.422099 \n",
      "\n",
      "Epoch 397\n",
      "-------------------------------\n",
      "loss: 0.298148  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.425162 \n",
      "\n",
      "Epoch 398\n",
      "-------------------------------\n",
      "loss: 0.310333  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.473227 \n",
      "\n",
      "Epoch 399\n",
      "-------------------------------\n",
      "loss: 0.353743  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.417860 \n",
      "\n",
      "Epoch 400\n",
      "-------------------------------\n",
      "loss: 0.277067  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.482445 \n",
      "\n",
      "Epoch 401\n",
      "-------------------------------\n",
      "loss: 0.401460  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.434305 \n",
      "\n",
      "Epoch 402\n",
      "-------------------------------\n",
      "loss: 0.314362  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.449745 \n",
      "\n",
      "Epoch 403\n",
      "-------------------------------\n",
      "loss: 0.362884  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.455729 \n",
      "\n",
      "Epoch 404\n",
      "-------------------------------\n",
      "loss: 0.346008  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.420516 \n",
      "\n",
      "Epoch 405\n",
      "-------------------------------\n",
      "loss: 0.303191  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.489432 \n",
      "\n",
      "Epoch 406\n",
      "-------------------------------\n",
      "loss: 0.379131  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.423002 \n",
      "\n",
      "Epoch 407\n",
      "-------------------------------\n",
      "loss: 0.304257  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.449819 \n",
      "\n",
      "Epoch 408\n",
      "-------------------------------\n",
      "loss: 0.339534  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.443616 \n",
      "\n",
      "Epoch 409\n",
      "-------------------------------\n",
      "loss: 0.332773  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.436150 \n",
      "\n",
      "Epoch 410\n",
      "-------------------------------\n",
      "loss: 0.294363  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.452815 \n",
      "\n",
      "Epoch 411\n",
      "-------------------------------\n",
      "loss: 0.370834  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.420938 \n",
      "\n",
      "Epoch 412\n",
      "-------------------------------\n",
      "loss: 0.307781  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.479890 \n",
      "\n",
      "Epoch 413\n",
      "-------------------------------\n",
      "loss: 0.370832  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.457529 \n",
      "\n",
      "Epoch 414\n",
      "-------------------------------\n",
      "loss: 0.350390  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.441497 \n",
      "\n",
      "Epoch 415\n",
      "-------------------------------\n",
      "loss: 0.317744  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.471373 \n",
      "\n",
      "Epoch 416\n",
      "-------------------------------\n",
      "loss: 0.379722  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.443961 \n",
      "\n",
      "Epoch 417\n",
      "-------------------------------\n",
      "loss: 0.321826  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.470573 \n",
      "\n",
      "Epoch 418\n",
      "-------------------------------\n",
      "loss: 0.383245  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.443179 \n",
      "\n",
      "Epoch 419\n",
      "-------------------------------\n",
      "loss: 0.303486  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.471500 \n",
      "\n",
      "Epoch 420\n",
      "-------------------------------\n",
      "loss: 0.354563  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.423838 \n",
      "\n",
      "Epoch 421\n",
      "-------------------------------\n",
      "loss: 0.302671  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.425484 \n",
      "\n",
      "Epoch 422\n",
      "-------------------------------\n",
      "loss: 0.302958  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.434977 \n",
      "\n",
      "Epoch 423\n",
      "-------------------------------\n",
      "loss: 0.304580  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.452315 \n",
      "\n",
      "Epoch 424\n",
      "-------------------------------\n",
      "loss: 0.341384  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.436892 \n",
      "\n",
      "Epoch 425\n",
      "-------------------------------\n",
      "loss: 0.332042  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.446921 \n",
      "\n",
      "Epoch 426\n",
      "-------------------------------\n",
      "loss: 0.344209  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.456738 \n",
      "\n",
      "Epoch 427\n",
      "-------------------------------\n",
      "loss: 0.331507  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.428733 \n",
      "\n",
      "Epoch 428\n",
      "-------------------------------\n",
      "loss: 0.294473  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.468171 \n",
      "\n",
      "Epoch 429\n",
      "-------------------------------\n",
      "loss: 0.341525  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.427313 \n",
      "\n",
      "Epoch 430\n",
      "-------------------------------\n",
      "loss: 0.326703  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.459164 \n",
      "\n",
      "Epoch 431\n",
      "-------------------------------\n",
      "loss: 0.313813  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.449648 \n",
      "\n",
      "Epoch 432\n",
      "-------------------------------\n",
      "loss: 0.315292  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.435701 \n",
      "\n",
      "Epoch 433\n",
      "-------------------------------\n",
      "loss: 0.322868  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.455634 \n",
      "\n",
      "Epoch 434\n",
      "-------------------------------\n",
      "loss: 0.307340  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.435388 \n",
      "\n",
      "Epoch 435\n",
      "-------------------------------\n",
      "loss: 0.296956  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.440714 \n",
      "\n",
      "Epoch 436\n",
      "-------------------------------\n",
      "loss: 0.317586  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.438607 \n",
      "\n",
      "Epoch 437\n",
      "-------------------------------\n",
      "loss: 0.297685  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.436317 \n",
      "\n",
      "Epoch 438\n",
      "-------------------------------\n",
      "loss: 0.292218  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.428176 \n",
      "\n",
      "Epoch 439\n",
      "-------------------------------\n",
      "loss: 0.304685  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.415805 \n",
      "\n",
      "Epoch 440\n",
      "-------------------------------\n",
      "loss: 0.282660  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.446457 \n",
      "\n",
      "Epoch 441\n",
      "-------------------------------\n",
      "loss: 0.346386  [ 1024/176128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Error: \n",
      " Avg loss: 0.448590 \n",
      "\n",
      "Epoch 442\n",
      "-------------------------------\n",
      "loss: 0.336440  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.417251 \n",
      "\n",
      "Epoch 443\n",
      "-------------------------------\n",
      "loss: 0.281878  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.451249 \n",
      "\n",
      "Epoch 444\n",
      "-------------------------------\n",
      "loss: 0.311995  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.437844 \n",
      "\n",
      "Epoch 445\n",
      "-------------------------------\n",
      "loss: 0.307039  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.423368 \n",
      "\n",
      "Epoch 446\n",
      "-------------------------------\n",
      "loss: 0.287223  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.438763 \n",
      "\n",
      "Epoch 447\n",
      "-------------------------------\n",
      "loss: 0.328052  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.425937 \n",
      "\n",
      "Epoch 448\n",
      "-------------------------------\n",
      "loss: 0.327206  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.422603 \n",
      "\n",
      "Epoch 449\n",
      "-------------------------------\n",
      "loss: 0.287013  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.433052 \n",
      "\n",
      "Epoch 450\n",
      "-------------------------------\n",
      "loss: 0.277643  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.421733 \n",
      "\n",
      "Epoch 451\n",
      "-------------------------------\n",
      "loss: 0.278243  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.426657 \n",
      "\n",
      "Epoch 452\n",
      "-------------------------------\n",
      "loss: 0.304739  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.423764 \n",
      "\n",
      "Epoch 453\n",
      "-------------------------------\n",
      "loss: 0.278100  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.441344 \n",
      "\n",
      "Epoch 454\n",
      "-------------------------------\n",
      "loss: 0.318902  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.424490 \n",
      "\n",
      "Epoch 455\n",
      "-------------------------------\n",
      "loss: 0.277469  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.431808 \n",
      "\n",
      "Epoch 456\n",
      "-------------------------------\n",
      "loss: 0.296372  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.423053 \n",
      "\n",
      "Epoch 457\n",
      "-------------------------------\n",
      "loss: 0.267829  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.442729 \n",
      "\n",
      "Epoch 458\n",
      "-------------------------------\n",
      "loss: 0.319277  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.435169 \n",
      "\n",
      "Epoch 459\n",
      "-------------------------------\n",
      "loss: 0.288703  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.427699 \n",
      "\n",
      "Epoch 460\n",
      "-------------------------------\n",
      "loss: 0.267741  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.419768 \n",
      "\n",
      "Epoch 461\n",
      "-------------------------------\n",
      "loss: 0.275574  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.414986 \n",
      "\n",
      "Epoch 462\n",
      "-------------------------------\n",
      "loss: 0.274678  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.433001 \n",
      "\n",
      "Epoch 463\n",
      "-------------------------------\n",
      "loss: 0.276309  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.423421 \n",
      "\n",
      "Epoch 464\n",
      "-------------------------------\n",
      "loss: 0.277634  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.420091 \n",
      "\n",
      "Epoch 465\n",
      "-------------------------------\n",
      "loss: 0.259114  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.443690 \n",
      "\n",
      "Epoch 466\n",
      "-------------------------------\n",
      "loss: 0.286065  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.423544 \n",
      "\n",
      "Epoch 467\n",
      "-------------------------------\n",
      "loss: 0.278252  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.418897 \n",
      "\n",
      "Epoch 468\n",
      "-------------------------------\n",
      "loss: 0.273294  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.421580 \n",
      "\n",
      "Epoch 469\n",
      "-------------------------------\n",
      "loss: 0.282242  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.429597 \n",
      "\n",
      "Epoch 470\n",
      "-------------------------------\n",
      "loss: 0.291481  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.414320 \n",
      "\n",
      "Epoch 471\n",
      "-------------------------------\n",
      "loss: 0.265920  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.434256 \n",
      "\n",
      "Epoch 472\n",
      "-------------------------------\n",
      "loss: 0.303268  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.459711 \n",
      "\n",
      "Epoch 473\n",
      "-------------------------------\n",
      "loss: 0.331131  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.429249 \n",
      "\n",
      "Epoch 474\n",
      "-------------------------------\n",
      "loss: 0.276882  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.439250 \n",
      "\n",
      "Epoch 475\n",
      "-------------------------------\n",
      "loss: 0.301432  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.464065 \n",
      "\n",
      "Epoch 476\n",
      "-------------------------------\n",
      "loss: 0.323202  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.426731 \n",
      "\n",
      "Epoch 477\n",
      "-------------------------------\n",
      "loss: 0.273896  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.417239 \n",
      "\n",
      "Epoch 478\n",
      "-------------------------------\n",
      "loss: 0.274629  [ 1024/176128]\n",
      "Test Error: \n",
      " Avg loss: 0.448184 \n",
      "\n",
      "Epoch 479\n",
      "-------------------------------\n",
      "loss: 0.308430  [ 1024/176128]\n"
     ]
    }
   ],
   "source": [
    "if optChoice == 'sgd':\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "elif optChoice == 'adam':\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learningRate)\n",
    "else:\n",
    "    optimizer = None\n",
    "    print('no optimizer chosen...')\n",
    "\n",
    "run = neptune.init_run(\n",
    "    project=neptuneProject,\n",
    "    api_token=api_token,  \n",
    "    capture_hardware_metrics=True,\n",
    "    capture_stderr=True,\n",
    "    capture_stdout=True,\n",
    ")\n",
    "\n",
    "PARAMS = {\n",
    "    \"batch_size\": batch_size,\n",
    "    \"learning_rate\": learningRate,\n",
    "    \"optimizer\": optChoice,\n",
    "    \"patience\": patience,\n",
    "    \"subsampleFreq\": subsampleFreq,\n",
    "    \"secondsInWindow\": secondsInWindow,\n",
    "    \"nperseg\": nperseg,\n",
    "    \"noverlap\": noverlap,\n",
    "    \"window\": stringify_unsupported(window),\n",
    "    \"loss_fn\": stringify_unsupported(loss_fn),\n",
    "    \"architectureString\": str(model),\n",
    "    \"numParameters\": sdm.count_parameters(model)\n",
    "}\n",
    "run[\"parameters\"] = PARAMS\n",
    "\n",
    "noImprovementCount = 0\n",
    "\n",
    "#epochs = 2\n",
    "\n",
    "try:\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loss = sdm.train(trainDataLoader, model, loss_fn, optimizer, device)\n",
    "        test_loss = sdm.test(validDataLoader, model, loss_fn, device)\n",
    "\n",
    "        if test_loss < bestTestLoss:\n",
    "            noImprovementCount = 0\n",
    "            bestTestLoss = test_loss\n",
    "            torch.save(model, modelPath)\n",
    "            run[\"model_best\"].upload(modelPath)\n",
    "            run[\"best_test_loss\"] =  bestTestLoss\n",
    "            run[\"best_test_epoch\"] = t\n",
    "            print(\"\\nSaved a new best model!\\n\")\n",
    "        else:\n",
    "            noImprovementCount = noImprovementCount + 1\n",
    "\n",
    "        run[\"train/loss\"].append(train_loss)\n",
    "        run[\"test/loss\"].append(test_loss)\n",
    "\n",
    "        if noImprovementCount >= patience:   \n",
    "            print(\"Early stopping invoked....\")\n",
    "            break\n",
    "\n",
    "    run.stop()\n",
    "    print(\"Done!\")\n",
    "except:\n",
    "    run.stop()\n",
    "    print(\"Training aborted...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac3474b",
   "metadata": {},
   "source": [
    "# PLOT RESULTS OF FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a858773f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT PREDICTION VERSUS TRUTH\n",
    "\n",
    "trainPlotFlag = True\n",
    "    \n",
    "if trainPlotFlag:\n",
    "    x = trainXTensor\n",
    "    trainTitle = 'train'\n",
    "else:\n",
    "    x = validXTensor\n",
    "    trainTitle = 'valididation'\n",
    "\n",
    "model.to('cpu')\n",
    "predict = model(x).cpu().detach().numpy()\n",
    "model.to(device)\n",
    "\n",
    "if predict.shape[1] == 1:\n",
    "    yPred = predict[:,0]\n",
    "    if trainPlotFlag:\n",
    "        yTrue = yTrainTimeDomain[:,0]\n",
    "    else:\n",
    "        yTrue = yValidTimeDomain[:,0]\n",
    "else:\n",
    "    _, yPred = realSTFTtoTimeSeries(predict)\n",
    "    if trainPlotFlag:\n",
    "        y = y_trainRTheta\n",
    "        _, yTrue = realSTFTtoTimeSeries(y)\n",
    "    else:\n",
    "        y = y_validRTheta\n",
    "        _, yTrue = realSTFTtoTimeSeries(y)\n",
    "        \n",
    "\n",
    "lossTemp = loss_fn(torch.tensor(yPred), torch.tensor(yTrue)).item()\n",
    "title = 'Data: ' + trainTitle + ' (loss: %s)' % str(lossTemp)\n",
    "plt.figure()\n",
    "plt.plot(yPred, label='predict')\n",
    "plt.plot(yTrue, label='true')\n",
    "plt.legend()\n",
    "plt.title(title)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e671c8d3",
   "metadata": {},
   "source": [
    "# SCRATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a8c14b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32., 32., 32., ..., 32., 32., 32.],\n",
       "       [32., 32., 32., ..., 32., 32., 32.],\n",
       "       [32., 32., 32., ..., 32., 32., 32.],\n",
       "       ...,\n",
       "       [32., 32., 32., ..., 32., 32., 32.],\n",
       "       [32., 32., 32., ..., 32., 32., 32.],\n",
       "       [32., 32., 32., ..., 32., 32., 32.]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.signal import spectrogram, stft, istft, check_NOLA\n",
    "\n",
    "fs = 1\n",
    "nperseg = 32\n",
    "noverlap = 31\n",
    "#windowType = ('tukey', .25)\n",
    "windowType = np.ones(nperseg)\n",
    "\n",
    "\n",
    "a = np.random.rand(100)\n",
    "f, t, S = stft(a, fs=fs, window=windowType, nperseg=nperseg, noverlap=noverlap)\n",
    "\n",
    "b = torch.stft(torch.tensor(a), \n",
    "               n_fft = nperseg, \n",
    "               hop_length = 1, \n",
    "               return_complex=True, \n",
    "               normalized=False, \n",
    "               onesided=True, \n",
    "               pad_mode='constant').numpy()\n",
    "\n",
    "np.abs(np.divide(b,S))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "98aa5ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://new-ui.neptune.ai/jettinger35/predictScalp/\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sys/id</th>\n",
       "      <th>best_test_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PRED-54</td>\n",
       "      <td>0.391573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PRED-53</td>\n",
       "      <td>0.423721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PRED-46</td>\n",
       "      <td>0.405430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PRED-43</td>\n",
       "      <td>0.390877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PRED-38</td>\n",
       "      <td>0.398955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>PRED-35</td>\n",
       "      <td>0.403722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>PRED-34</td>\n",
       "      <td>0.417772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>PRED-32</td>\n",
       "      <td>0.408214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>PRED-31</td>\n",
       "      <td>0.409690</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sys/id  best_test_loss\n",
       "0  PRED-54        0.391573\n",
       "1  PRED-53        0.423721\n",
       "2  PRED-46        0.405430\n",
       "3  PRED-43        0.390877\n",
       "4  PRED-38        0.398955\n",
       "5  PRED-35        0.403722\n",
       "6  PRED-34        0.417772\n",
       "7  PRED-32        0.408214\n",
       "8  PRED-31        0.409690"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# HOW TO GRAB DATA FROM NEPTUNE\n",
    "\n",
    "project = neptune.init_project(project=\"jettinger35/predictScalp\")\n",
    "df = project.fetch_runs_table().to_pandas()\n",
    "df[['sys/id','best_test_loss']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14d95ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import get_window\n",
    "a = get_window(('tukey', .25), nperseg)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab0c6db",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        return loss\n",
    "            \n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "    test_loss /= num_batches\n",
    "    print(f\"Test Error: \\n Avg loss: {test_loss:>8f} \\n\")\n",
    "    return test_loss\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    \n",
    "    def __init__(self, layerOrderedDict):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(layerOrderedDict)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "    \n",
    "    \n",
    "# GIVEN A LIST OF LAYER SIZES MAKE AN ORDERED DICTIONARY FOR INITIALIZING A PYTORCH NET\n",
    "\n",
    "def listToOrderedDict(sizeList):\n",
    "    n = len(sizeList)\n",
    "    tupleList = []\n",
    "    for i in range(n - 1):\n",
    "        tupleList.append(('bn%s' % str(i), nn.BatchNorm1d(sizeList[i])))\n",
    "        tupleList.append(('l%s' % str(i), nn.Linear(sizeList[i], sizeList[i+1])))\n",
    "        tupleList.append(('r%s' % str(i), nn.ReLU()))\n",
    "    return OrderedDict(tupleList[:-1])\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "'''\n",
    "\n",
    "'''\n",
    "    layerSizeList = [trainXTensor.shape[1]] + hiddenLayerSizes + [trainYTensor.shape[1]]\n",
    "    layerOrderedDict = sdm.listToOrderedDict(layerSizeList)\n",
    "    model = sdm.NeuralNetwork(layerOrderedDict)\n",
    "    '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch-1.13",
   "language": "python",
   "name": "pytorch-1.13"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

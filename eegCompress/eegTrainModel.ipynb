{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "li7HX-beZFGg",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'torchModels' from '/home/jmark.ettinger/github/uFlorida/eegCompress/torchModels.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import neptune\n",
    "from neptune_pytorch import NeptuneLogger\n",
    "from neptune.utils import stringify_unsupported\n",
    "\n",
    "import datetime\n",
    "import pytz\n",
    "timeZone = pytz.timezone('America/Los_Angeles')\n",
    "from operator import itemgetter\n",
    "\n",
    "from eegUtils import *\n",
    "import torchModels\n",
    "from myUtils import *\n",
    "\n",
    "import importlib\n",
    "importlib.reload(torchModels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGXT2d4lB0n5"
   },
   "source": [
    "# Read the data from disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z78SjRnaX9bF",
    "outputId": "439db100-5aa5-4b49-850d-6b0cf3135775",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been moved to GPU\n",
      "(19, 1100367)\n"
     ]
    }
   ],
   "source": [
    "dataMultiply = 10**5\n",
    "sFreq = 256\n",
    "arrayInCompressedFile = 'arr_7'\n",
    "\n",
    "data = np.load('/blue/gkalamangalam/jmark.ettinger/eegCompress/processedData/elimPeaksSVD001.npz')[arrayInCompressedFile]\n",
    "nChannel, nSample = data.shape\n",
    "data = (data * dataMultiply).astype('float32')\n",
    "dataTensor = torch.tensor(data)\n",
    "if torch.cuda.is_available():\n",
    "    dataTensor = dataTensor.to('cuda')\n",
    "    print(\"Data has been moved to GPU\")\n",
    "else:\n",
    "    print(\"Data is on CPU\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = '/blue/gkalamangalam/jmark.ettinger/eegCompress/processedData/kmeansModels/kmeansModel_001_block7_1stack.npz'\n",
    "npzfile = np.load(path)\n",
    "centroids = npzfile['arr_0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "byjCmPBeZFGo"
   },
   "source": [
    "# Define Model, Optimizer, DataSet and Optionally Load All"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OGgAiuo8ZFGo",
    "outputId": "f1a3ae86-dfe2-40c6-f3b5-364f07b648bb",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size: 0.119MB\n",
      "conv1dKmeans(\n",
      "  (myNet): Sequential(\n",
      "    (0): Conv1d(19, 50, kernel_size=(3,), stride=(1,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv1d(50, 50, kernel_size=(3,), stride=(1,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv1d(50, 50, kernel_size=(3,), stride=(1,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "    (6): Flatten(start_dim=1, end_dim=-1)\n",
      "    (7): Linear(in_features=700, out_features=19, bias=True)\n",
      "  )\n",
      ")\n",
      "Model has been moved to GPU\n"
     ]
    }
   ],
   "source": [
    "# Set parameters\n",
    "modelType = 'kmeans'\n",
    "loadBool = 0\n",
    "#modelPath = '/content/drive/MyDrive/NeuroResearch/Data/eegCompress/models/savedModel_03-28 19:04_-0.034.pt'\n",
    "numSampleInput = 20\n",
    "numSampleOutput = 1\n",
    "\n",
    "\n",
    "initDict = {'kmeansInit': centroids, 'dataTensor': dataTensor, 'numSampleInput':numSampleInput}\n",
    "\n",
    "model, dataset, loss_function = torchModels.makeModel(modelType, initDict)\n",
    "# modelType, nChannel, numSampleInput, numSampleOutput, dataTensor\n",
    "\n",
    "sizeOfModel = modelSize(model)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.0)\n",
    "#optimizer = torch.optim.Adam(model.parameters())#, lr = 1e-1, weight_decay = 1e-8)\n",
    "\n",
    "if loadBool:\n",
    "    model, optimizer, totalEpoch, loss = loadModel(modelPath, model, optimizer, trainBool=True)\n",
    "    print(\"Model has been loaded: \" + modelPath)\n",
    "else:\n",
    "    totalEpoch = 0\n",
    "\n",
    "print(model)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.to('cuda')\n",
    "    print(\"Model has been moved to GPU\")\n",
    "else:\n",
    "    print(\"Model is on CPU\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WWeZ4PRYCGWR",
    "outputId": "7b4deb16-8cc9-4416-a7bb-a7d0ce4ba240",
    "tags": []
   },
   "outputs": [],
   "source": [
    "logFlag = False\n",
    "\n",
    "if logFlag:\n",
    "  run = neptune.init_run(\n",
    "      project=\"jettinger35/eegCompress\",\n",
    "      api_token=\"eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiIzMjFlMzY2MS1iOWZiLTRmZWEtOGMwNy0zOTVkMTljOGVjYTMifQ==\",\n",
    "      #with_id=\"EEG-116\"\n",
    "      )\n",
    "\n",
    "  npt_logger = NeptuneLogger(\n",
    "      run=run,\n",
    "      model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uvj5MCOA1ASz"
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3oawEnUlXEOO",
    "outputId": "93ffa385-10e2-4b73-9270-a42c5b0ae4c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "04-14 13:33: Epoch: 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 29\u001b[39m\n\u001b[32m     26\u001b[39m loss = loss_function(prediction, label)\n\u001b[32m     28\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m29\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m max_norm = \u001b[32m1.0\u001b[39m\n\u001b[32m     32\u001b[39m torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/gkalamangalam/jmark.ettinger/.conda/envs/myPytorch/lib/python3.11/site-packages/torch/_tensor.py:522\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    512\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    513\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    514\u001b[39m         Tensor.backward,\n\u001b[32m    515\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    520\u001b[39m         inputs=inputs,\n\u001b[32m    521\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m522\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    523\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    524\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/gkalamangalam/jmark.ettinger/.conda/envs/myPytorch/lib/python3.11/site-packages/torch/autograd/__init__.py:259\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    250\u001b[39m inputs = (\n\u001b[32m    251\u001b[39m     (inputs,)\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, (torch.Tensor, graph.GradientEdge))\n\u001b[32m   (...)\u001b[39m\u001b[32m    255\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m()\n\u001b[32m    256\u001b[39m )\n\u001b[32m    258\u001b[39m grad_tensors_ = _tensor_or_tensors_to_tuple(grad_tensors, \u001b[38;5;28mlen\u001b[39m(tensors))\n\u001b[32m--> \u001b[39m\u001b[32m259\u001b[39m grad_tensors_ = \u001b[43m_make_grads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_grads_batched\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m retain_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    261\u001b[39m     retain_graph = create_graph\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/blue/gkalamangalam/jmark.ettinger/.conda/envs/myPytorch/lib/python3.11/site-packages/torch/autograd/__init__.py:142\u001b[39m, in \u001b[36m_make_grads\u001b[39m\u001b[34m(outputs, grads, is_grads_batched)\u001b[39m\n\u001b[32m    136\u001b[39m         msg = (\n\u001b[32m    137\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mgrad can be implicitly created only for real scalar outputs\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    138\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout.dtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    139\u001b[39m         )\n\u001b[32m    140\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[32m    141\u001b[39m     new_grads.append(\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m         \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mones_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     )\n\u001b[32m    144\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    145\u001b[39m     new_grads.append(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mRuntimeError\u001b[39m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "epochs = 5\n",
    "batchSize = 32\n",
    "numRandomPlot = 2\n",
    "secondsToPlot = 5\n",
    "saveEveryNEpochs = 0 # 0 for no saving\n",
    "\n",
    "# initialize\n",
    "breakFlag = False\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=False, sampler=None)\n",
    "\n",
    "# training loop\n",
    "for epoch in range(epochs):\n",
    "    counter = 0\n",
    "\n",
    "    myPrint(\"Epoch: \" + str(totalEpoch))\n",
    "    if breakFlag:\n",
    "        myPrint(\"Break!\")\n",
    "        break\n",
    "    for (thisBlock, label) in loader:\n",
    "        counter += 1\n",
    "        prediction = model(thisBlock)\n",
    "        if np.any(np.isnan(prediction.detach().cpu().numpy())):\n",
    "            myPrint(\"NaN detected.  Counter: \" + str(counter))\n",
    "            breakFlag = True\n",
    "            break\n",
    "        loss = loss_function(prediction, label)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        max_norm = 1.0\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)\n",
    "\n",
    "        optimizer.step()\n",
    "        if logFlag:\n",
    "            run[npt_logger.base_namespace][\"train/log_loss\"].append(np.log(loss.item()))\n",
    "\n",
    "    if logFlag:\n",
    "      # plot random locations for original and predicted for comparison\n",
    "      for i in range(numRandomPlot):\n",
    "\n",
    "          startPlot = random.randint(0, nSample - (secondsToPlot * sFreq))\n",
    "          fig, original, predicted = timeSeriesCompare(model, startPlot, secondsToPlot, sFreq, data, numSampleInput)\n",
    "          plt.title(\"Epoch, Start, Blocks: \" + str((totalEpoch, startPlot, secondsToPlot)))\n",
    "          run[\"fig\"].append(fig)\n",
    "          plt.close()\n",
    "\n",
    "    totalEpoch += 1\n",
    "\n",
    "    print(\"Predicting and generating sampler...\")\n",
    "    sampler, predicted, residualMeasure = samplerMake(model, numSampleInput, data)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=False, sampler=sampler)\n",
    "\n",
    "    if saveEveryNEpochs > 0 and (epoch + 1) % saveEveryNEpochs == 0:\n",
    "      saveModel(model, optimizer, totalEpoch, loss, predicted)\n",
    "\n",
    "myPrint(\"Finished training...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aPNgp4gW_5W"
   },
   "source": [
    "# Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_LLbQ5rDW-tA",
    "outputId": "4c4ab8b4-a7ae-4b24-ee5d-b88d8ef9aeec"
   },
   "outputs": [],
   "source": [
    "sampler, predicted, residualMeasure = samplerMake(model, numSampleInput, data)\n",
    "saveModel(model, optimizer, totalEpoch, loss, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjGv6LJS0u06"
   },
   "source": [
    "# Compare the time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "8MX4N__GVAWa",
    "outputId": "f50ef3ad-bc9d-4998-f797-3cfd7e821658"
   },
   "outputs": [],
   "source": [
    "plotBool = 1\n",
    "\n",
    "if plotBool:\n",
    "  startPlot = 1000\n",
    "  secondsToPlot = 5\n",
    "  channel = 0\n",
    "  plotOption = 'both'\n",
    "\n",
    "  fig, original, predicted64 = timeSeriesCompare(model, startPlot, secondsToPlot, sFreq, data, numSampleInput, channel, plotOption)\n",
    "  plt.show(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QiajJ-Hq1f6m"
   },
   "source": [
    "# Save original and predicted data for local graphical comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-8PRONaU1e13"
   },
   "outputs": [],
   "source": [
    "predicted = predictEEG(model, None, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kap82C5COj2s"
   },
   "outputs": [],
   "source": [
    "path = '/content/drive/MyDrive/NeuroResearch/Data/eegCompress/processedData/origAndPredictedSVD001_block7.npz'\n",
    "dataToSaveList = [data, predicted]\n",
    "np.savez_compressed(path, *dataToSaveList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mxVAcHWqPbt8"
   },
   "source": [
    "# Show network parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dTYLWBOCfp9W",
    "outputId": "61763a4e-86c1-429e-820e-e77cc1a7592a"
   },
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    print((param.shape, param.device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhhtZuYDBCP6"
   },
   "source": [
    "# Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPORT ONNX FOR VISUALIZATION IN NETRON APP\n",
    "\n",
    "visualizationPath = '/content/drive/MyDrive/NeuroResearch/Data/eegCompress/models/model.onnx'\n",
    "dataset = datasetMake(torch.tensor(data[:,100:200]), model.numSampleInput, model.typeCode)\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, sampler=None)\n",
    "batch, label = next(loader.__iter__())\n",
    "yhat = model(batch)\n",
    "\n",
    "torch.onnx.export(model, batch, f=visualizationPath)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "myPytorch",
   "language": "python",
   "name": "mypytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

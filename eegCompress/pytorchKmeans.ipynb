{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "O7KOU3RctXpJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "import numpy as np\n",
    "import cuml\n",
    "from cuml import KMeans\n",
    "from cuml.cluster import KMeans\n",
    "import cudf\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "import pytz\n",
    "timeZone = pytz.timezone('America/Los_Angeles')\n",
    "\n",
    "import torch\n",
    "\n",
    "import myUtils\n",
    "import torchModels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reconstruct(toPredict, predictorModel, kmeansModel, nStack):\n",
    "    predicted = np.zeros_like(toPredict)\n",
    "    predicted[:,1] = toPredict[:,1]\n",
    "\n",
    "    #stackedSamples, _ = residualStack.shape\n",
    "    _, nSamples = toPredict.shape\n",
    "    counter = 1\n",
    "\n",
    "    while counter + nStack <= nSamples:\n",
    "        if counter % 100000 == 0:\n",
    "            myPrint(\"predicting sample: \" + str(counter))\n",
    "\n",
    "        thisToPredict = toPredict[:, counter:counter + nStack].flatten(order='F')\n",
    "        thisPredictor = np.repeat(predicted[:, counter - 1], nStack)\n",
    "        thisResidual = thisToPredict - thisPredictor\n",
    "        \n",
    "        #kmeansModel.fit(np.expand_dims(thisResidual,0))\n",
    "        \n",
    "        index = int(kmeans.predict(np.expand_dims(thisResidual, 0))[0])\n",
    "        thisCentroid = centroids[index,:]\n",
    "        for j in range(nStack):\n",
    "            predicted[:, counter] = data[:, counter - 1 - j] + thisCentroid[j * nChannel: (j+1) * nChannel].transpose()\n",
    "            counter += 1\n",
    "            \n",
    "    return predicted, kmeansModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JNQ1lHLY3Ij4",
    "outputId": "6ab2c55f-12de-4df9-ef9c-eb99a04cb0a4",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data has been moved to GPU\n",
      "(19, 1100367)\n"
     ]
    }
   ],
   "source": [
    "dataMultiply = 10**5\n",
    "sFreq = 256\n",
    "arrayInCompressedFile = 'arr_7'\n",
    "\n",
    "data = np.load('/blue/gkalamangalam/jmark.ettinger/eegCompress/processedData/elimPeaksSVD001.npz')[arrayInCompressedFile]\n",
    "nChannel, nSample = data.shape\n",
    "data = (data * dataMultiply).astype('float32')\n",
    "dataTensor = torch.tensor(data)\n",
    "if torch.cuda.is_available():\n",
    "    dataTensor = dataTensor.to('cuda')\n",
    "    print(\"Data has been moved to GPU\")\n",
    "else:\n",
    "    print(\"Data is on CPU\")\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set parameters\n",
    "modelType = 'kmeans'\n",
    "loadBool = 1\n",
    "modelPath = '/blue/gkalamangalam/jmark.ettinger/eegCompress/models/savedModel_04-16 00:52_-0.023.pt'\n",
    "numSampleInput = 20\n",
    "numSampleOutput = 1\n",
    "\n",
    "path = '/blue/gkalamangalam/jmark.ettinger/eegCompress/processedData/kmeansModels/kmeansModel_001_block7_1stack.npz'\n",
    "npzfile = np.load(path)\n",
    "centroids = npzfile['arr_0']\n",
    "initDict = {'kmeansInit': centroids, 'dataTensor': dataTensor, 'numSampleInput':numSampleInput}\n",
    "\n",
    "model, dataset, loss_function = torchModels.makeModel(modelType, initDict)\n",
    "model = model.to('cuda')\n",
    "numSampleInput = model.numSampleInput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batchSize = 1\n",
    "\n",
    "centroids = model.kmeans.float().detach().cpu().numpy()\n",
    "nCentroids,_ = centroids.shape\n",
    "kmeans = KMeans(n_clusters=nCentroids, init=centroids, n_init=1)\n",
    "\n",
    "loader = torch.utils.data.DataLoader(dataset, batch_size=batchSize, shuffle=False, sampler=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reconstruct the data from predictions and centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "#def reconstruct(toPredict, predictorModel, kmeansModel, nStack):\n",
    "\n",
    "nStack = 1\n",
    "toPredict = dataTensor\n",
    "predictionModel = model\n",
    "kmeansModel = kmeans\n",
    "\n",
    "model.eval()\n",
    "predicted = np.zeros_like(toPredict.detach().cpu().numpy())\n",
    "predicted[:,0:numSampleInput] = toPredict[:,0:numSampleInput].detach().cpu().numpy()\n",
    "\n",
    "counter = numSampleInput\n",
    "for (thisBlock, label) in loader:\n",
    "    if counter % 100000 == 0:\n",
    "        print(counter)\n",
    "    thisPrediction = predictionModel(thisBlock)\n",
    "    thisResidual = (label - thisPrediction).detach().cpu().numpy().transpose()\n",
    "    \n",
    "    index = int(kmeans.predict(np.expand_dims(thisResidual, 0))[0])\n",
    "    thisCentroid = centroids[index,:]\n",
    "    predicted[:,counter] = thisCentroid\n",
    "    counter += 1\n",
    "    \n",
    "    for j in range(nStack):\n",
    "        predicted[:, counter] = data[:, counter - 1 - j] + thisCentroid[j * nChannel: (j+1) * nChannel].transpose()\n",
    "        counter += 1\n",
    "\n",
    "model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "np.mean(np.abs(data - predicted)), np.max(np.abs(data - predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " current best: (0.044626124, 0.6939485)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "channel = 0\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(predicted[channel,:], label='predicted')\n",
    "plt.plot(data[channel,:], label='original')\n",
    "residual = data - predicted\n",
    "#plt.plot(residual[channel,:])\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save original and predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vImqGqzJS8Dh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "path = '/blue/gkalamangalam/jmark.ettinger/eegCompress/processedData/origAndPredictedLossy.npz'\n",
    "dataToSaveList = [data[:,0:failureIndex], predicted[:, 0:failureIndex]]\n",
    "np.savez_compressed(path, *dataToSaveList)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jaK1mJneWd4k"
   },
   "source": [
    "# Scratch"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "rapidsAndPytorch",
   "language": "python",
   "name": "rapidsandpytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
